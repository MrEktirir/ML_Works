{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11. Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glorot and He Initialization\n",
    "* By default, Keras uses Glorot initialization with a uniform distribution. When you create a layer, you can switch to He initialization by setting kernel_initializer=\"he_uniform\" or kernel_initializer=\"he_normal\" like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Initialization | Activation functions                 | σ² (Normal)      |\n",
    "|----------------|--------------------------------------|------------------|\n",
    "| Glorot         | None, tanh, sigmoid, softmax        | 1 / fan_avg      |\n",
    "| He             | ReLU, Leaky ReLU, ELU, GELU, Swish, Mish | 2 / fan_in       |\n",
    "| LeCun          | SELU                                | 1 / fan_in       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dense = tf.keras.layers.Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Alternatively, you can obtain any of the initializations listed in **Table**  and more using the `VarianceScaling` initializer. For example, if you want  **He initialization** with a uniform distribution and based on *fan_avg*  (rather than *fan_in*), you can use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_avg_init = tf.keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\", distribution=\"uniform\")\n",
    "dense = tf.keras.layers.Dense(\n",
    "    50,\n",
    "    activation=\"sigmoid\",\n",
    "    kernel_initializer=he_avg_init #Here it is...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaky ReLU\n",
    "\n",
    "* Keras includes the classes LeakyReLU and PReLU in the tf.keras.layers package. Just like for other ReLU variants, you should use He initialization with these. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_Relu = tf.keras.layers.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "dense = tf.keras.layers.Dense(50, activation=leaky_Relu, kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you prefer, you can also use LeakyReLU as a separate layer in your model; it makes no difference for training and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential([\n",
    "#     [...], # more layers\n",
    "#     tf.keras.layers.Dense(50, kernel_initializer=\"he_normal\"), # no activation\n",
    "#     tf.keras.layers.LeakyReLU(alpha=0.2), # activation as a separate layer\n",
    "#     [...] # more layers\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELU and SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELU\n",
    "dense = tf.keras.layers.Dense(50, activation=\"elu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELU\n",
    "dense = tf.keras.layers.Dense(50, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
