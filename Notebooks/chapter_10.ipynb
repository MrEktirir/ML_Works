{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Biological to Artificial Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron\n",
    "\n",
    "* The perceptron is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt. It is based on a slightly different artificial neuron called a threshold logic unit (TLU), or sometimes a linear threshold unit (LTU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scikit-Learn provides a Perceptron class that can be used pretty much as you would expect—for example, on the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = (iris.target == 0) # Iris setosa\n",
    "\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "X_new = [[2, 0.5], [3, 1]]\n",
    "y_pred = per_clf.predict(X_new) # predicts True and False for these 2 flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data,\n",
    "    housing.target,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full,\n",
    "    y_train_full,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_reg = MLPRegressor(\n",
    "    hidden_layer_sizes=[50, 50, 50],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    mlp_reg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = root_mean_squared_error(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5053326657967967"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "A = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(A.data, A.target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.90357396\n",
      "Validation score: 0.333333\n",
      "Iteration 2, loss = 0.90026353\n",
      "Validation score: 0.333333\n",
      "Iteration 3, loss = 0.89697773\n",
      "Validation score: 0.333333\n",
      "Iteration 4, loss = 0.89371710\n",
      "Validation score: 0.333333\n",
      "Iteration 5, loss = 0.89049354\n",
      "Validation score: 0.333333\n",
      "Iteration 6, loss = 0.88729736\n",
      "Validation score: 0.333333\n",
      "Iteration 7, loss = 0.88412733\n",
      "Validation score: 0.333333\n",
      "Iteration 8, loss = 0.88098044\n",
      "Validation score: 0.333333\n",
      "Iteration 9, loss = 0.87786561\n",
      "Validation score: 0.333333\n",
      "Iteration 10, loss = 0.87478224\n",
      "Validation score: 0.333333\n",
      "Iteration 11, loss = 0.87170958\n",
      "Validation score: 0.333333\n",
      "Iteration 12, loss = 0.86867681\n",
      "Validation score: 0.333333\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "mlp_clf = MLPClassifier(\n",
    "    hidden_layer_sizes=[5,8],\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    verbose = True,\n",
    "    alpha=0.01,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    mlp_clf\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15,  0,  0],\n",
       "       [ 7,  4,  0],\n",
       "       [ 2, 10,  0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing MLPs with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Image Classifier Using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Keras to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's look data shapes & types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For simplicity, we’ll scale the pixel intensities down to the 0–1 range by dividing them by 255.0 (this also converts them to floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = X_train/255.0, X_valid/255.0, X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With MNIST, when the label is equal to 5, it means that the image represents the handwritten digit 5. Easy. For Fashion MNIST, however, we need the list of class names to know what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For example, the first image in the training set represents an ankle boot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ankle boot'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model using the sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now let’s build the neural network! Here is a classification MLP with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=[28, 28]))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(300, activation=\"relu\")) #Hidden 1\n",
    "model.add(tf.keras.layers.Dense(100, activation=\"relu\")) #Hidden 2\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instead of adding the layers one by one as we just did, it’s often more convenient to pass a list of layers when creating the Sequential model. You can also drop the Input layer and instead specify the input_shape in the first layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model = tf.keras.Sequential(\n",
    "#    [\n",
    "#        tf.keras.layers.Input(shape=[28, 28]),\n",
    "#        tf.keras.layers.Dense(300, activation=\"relu\"), #Hidden 1\n",
    "#        tf.keras.layers.Dense(100, activation=\"relu\"), #Hidden 2\n",
    "#        tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "#    ]\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model’s summary() method displays all the model’s layers,⁠ including each layer’s name (which is automatically generated unless you set it when creating the layer), its output shape (None means the batch size can be anything), and its number of parameters. The summary ends with the total number of parameters, including trainable and non-trainable parameters. Here we only have trainable parameters.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that Dense layers often have a lot of parameters. For example, the first hidden layer has 784 × 300 connection weights, plus 300 bias terms, which adds up to 235,500 parameters! This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting, especially when you do not have a lot of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can easily get a model’s list of layers using the layers attribute, or use the get_layer() method to access a layer by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Flatten name=flatten, built=True>,\n",
       " <Dense name=dense, built=True>,\n",
       " <Dense name=dense_1, built=True>,\n",
       " <Dense name=dense_2, built=True>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense') is hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.4548681e-03,  6.7301780e-02, -5.4853179e-02, ...,\n",
       "        -1.0532580e-02, -2.9738259e-02,  5.3919852e-02],\n",
       "       [-5.2299351e-02,  4.4727929e-02, -2.8072014e-02, ...,\n",
       "         2.8820023e-02,  2.5427625e-02, -5.5640936e-05],\n",
       "       [-6.5545946e-02,  4.8573785e-02, -5.4146536e-02, ...,\n",
       "         3.1672746e-02, -3.0590791e-02,  3.0236386e-02],\n",
       "       ...,\n",
       "       [ 3.4629099e-02, -4.9974274e-02,  2.1171346e-03, ...,\n",
       "        -5.2962288e-02,  4.0621549e-02, -3.0246962e-02],\n",
       "       [-1.5180375e-02,  3.5097286e-02,  1.4130190e-02, ...,\n",
       "        -6.3042499e-02,  3.4199171e-02,  1.9880779e-02],\n",
       "       [-6.1947610e-02,  5.2647069e-02, -3.6660403e-02, ...,\n",
       "        -1.5093584e-02, -5.9872389e-02, -1.9342721e-02]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer = \"sgd\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - accuracy: 0.6770 - loss: 0.9981 - val_accuracy: 0.8312 - val_loss: 0.4986\n",
      "Epoch 2/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 19ms/step - accuracy: 0.8278 - loss: 0.4989 - val_accuracy: 0.8424 - val_loss: 0.4524\n",
      "Epoch 3/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 21ms/step - accuracy: 0.8460 - loss: 0.4463 - val_accuracy: 0.8472 - val_loss: 0.4293\n",
      "Epoch 4/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18ms/step - accuracy: 0.8541 - loss: 0.4168 - val_accuracy: 0.8518 - val_loss: 0.4140\n",
      "Epoch 5/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 17ms/step - accuracy: 0.8619 - loss: 0.3959 - val_accuracy: 0.8540 - val_loss: 0.4015\n",
      "Epoch 6/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 14ms/step - accuracy: 0.8665 - loss: 0.3794 - val_accuracy: 0.8566 - val_loss: 0.3926\n",
      "Epoch 7/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 13ms/step - accuracy: 0.8707 - loss: 0.3658 - val_accuracy: 0.8582 - val_loss: 0.3845\n",
      "Epoch 8/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.8752 - loss: 0.3538 - val_accuracy: 0.8606 - val_loss: 0.3771\n",
      "Epoch 9/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 14ms/step - accuracy: 0.8780 - loss: 0.3433 - val_accuracy: 0.8618 - val_loss: 0.3705\n",
      "Epoch 10/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 12ms/step - accuracy: 0.8815 - loss: 0.3336 - val_accuracy: 0.8646 - val_loss: 0.3657\n",
      "Epoch 11/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.8847 - loss: 0.3246 - val_accuracy: 0.8676 - val_loss: 0.3615\n",
      "Epoch 12/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.8875 - loss: 0.3164 - val_accuracy: 0.8694 - val_loss: 0.3566\n",
      "Epoch 13/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.8898 - loss: 0.3088 - val_accuracy: 0.8706 - val_loss: 0.3526\n",
      "Epoch 14/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.8923 - loss: 0.3017 - val_accuracy: 0.8724 - val_loss: 0.3498\n",
      "Epoch 15/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 15ms/step - accuracy: 0.8944 - loss: 0.2949 - val_accuracy: 0.8724 - val_loss: 0.3487\n",
      "Epoch 16/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 13ms/step - accuracy: 0.8966 - loss: 0.2885 - val_accuracy: 0.8734 - val_loss: 0.3465\n",
      "Epoch 17/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.8992 - loss: 0.2826 - val_accuracy: 0.8742 - val_loss: 0.3443\n",
      "Epoch 18/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 12ms/step - accuracy: 0.9005 - loss: 0.2767 - val_accuracy: 0.8748 - val_loss: 0.3423\n",
      "Epoch 19/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.9025 - loss: 0.2712 - val_accuracy: 0.8758 - val_loss: 0.3409\n",
      "Epoch 20/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.9052 - loss: 0.2657 - val_accuracy: 0.8748 - val_loss: 0.3403\n",
      "Epoch 21/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.9070 - loss: 0.2607 - val_accuracy: 0.8766 - val_loss: 0.3405\n",
      "Epoch 22/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.9088 - loss: 0.2557 - val_accuracy: 0.8772 - val_loss: 0.3383\n",
      "Epoch 23/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9107 - loss: 0.2509 - val_accuracy: 0.8778 - val_loss: 0.3384\n",
      "Epoch 24/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.9123 - loss: 0.2464 - val_accuracy: 0.8790 - val_loss: 0.3373\n",
      "Epoch 25/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.9138 - loss: 0.2418 - val_accuracy: 0.8790 - val_loss: 0.3360\n",
      "Epoch 26/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.9157 - loss: 0.2375 - val_accuracy: 0.8798 - val_loss: 0.3364\n",
      "Epoch 27/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.9175 - loss: 0.2335 - val_accuracy: 0.8812 - val_loss: 0.3361\n",
      "Epoch 28/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.9194 - loss: 0.2292 - val_accuracy: 0.8812 - val_loss: 0.3367\n",
      "Epoch 29/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.9209 - loss: 0.2251 - val_accuracy: 0.8816 - val_loss: 0.3351\n",
      "Epoch 30/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.9219 - loss: 0.2211 - val_accuracy: 0.8832 - val_loss: 0.3346\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The fit() method returns a History object containing the training parameters (history.params), the list of epochs it went through (history.epoch), and most importantly a dictionary (history.history) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set (if any). If you use this dictionary to create a Pandas DataFrame and call its plot() method, you get the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Epoch'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8, 5),\n",
    "    xlim=[0, 29],\n",
    "    ylim=[0, 1],\n",
    "    grid=True,\n",
    "    xlabel=\"Epoch\",\n",
    "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8754 - loss: 0.3612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35980477929115295, 0.8751999735832214]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model to Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.36, 0.  , 0.01, 0.  , 0.63],\n",
       "       [0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you only care about the class with the highest estimated probability (even if that probability is quite low), then you can use the argmax() method to get the highest probability class index for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = y_proba.argmax(axis=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Regression MLP Using the Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let’s switch back to the California housing problem and tackle it using the same MLP as earlier, with 3 hidden layers composed of 50 neurons each, but this time building it with Keras. Using the sequential API to build, train, evaluate, and use a regression MLP is quite similar to what we did for classification. The main differences in the following code example are the fact that the output layer has a single neuron (since we only want to predict a single value) and it uses no activation function, the loss function is the mean squared error, the metric is the RMSE, and we’re using an Adam optimizer like Scikit-Learn’s MLPRegressor did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MRE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - RootMeanSquaredError: 1.1444 - loss: 1.3995 - val_RootMeanSquaredError: 0.6262 - val_loss: 0.3922\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - RootMeanSquaredError: 0.6296 - loss: 0.3968 - val_RootMeanSquaredError: 0.6251 - val_loss: 0.3908\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - RootMeanSquaredError: 0.6017 - loss: 0.3622 - val_RootMeanSquaredError: 0.7287 - val_loss: 0.5311\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - RootMeanSquaredError: 0.5870 - loss: 0.3448 - val_RootMeanSquaredError: 0.7732 - val_loss: 0.5978\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - RootMeanSquaredError: 0.5784 - loss: 0.3346 - val_RootMeanSquaredError: 0.9553 - val_loss: 0.9126\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - RootMeanSquaredError: 0.5699 - loss: 0.3249 - val_RootMeanSquaredError: 1.1431 - val_loss: 1.3066\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - RootMeanSquaredError: 0.5647 - loss: 0.3190 - val_RootMeanSquaredError: 0.8642 - val_loss: 0.7468\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - RootMeanSquaredError: 0.5577 - loss: 0.3111 - val_RootMeanSquaredError: 0.6494 - val_loss: 0.4218\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - RootMeanSquaredError: 0.5506 - loss: 0.3033 - val_RootMeanSquaredError: 0.5410 - val_loss: 0.2927\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - RootMeanSquaredError: 0.5464 - loss: 0.2986 - val_RootMeanSquaredError: 0.8343 - val_loss: 0.6961\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - RootMeanSquaredError: 0.5457 - loss: 0.2979 - val_RootMeanSquaredError: 0.8596 - val_loss: 0.7389\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - RootMeanSquaredError: 0.5427 - loss: 0.2946 - val_RootMeanSquaredError: 0.9872 - val_loss: 0.9746\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - RootMeanSquaredError: 0.5399 - loss: 0.2915 - val_RootMeanSquaredError: 1.4622 - val_loss: 2.1380\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - RootMeanSquaredError: 0.5380 - loss: 0.2895 - val_RootMeanSquaredError: 0.8573 - val_loss: 0.7349\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - RootMeanSquaredError: 0.5337 - loss: 0.2849 - val_RootMeanSquaredError: 0.6842 - val_loss: 0.4682\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - RootMeanSquaredError: 0.5285 - loss: 0.2794 - val_RootMeanSquaredError: 0.6798 - val_loss: 0.4621\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - RootMeanSquaredError: 0.5256 - loss: 0.2764 - val_RootMeanSquaredError: 0.9295 - val_loss: 0.8640\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - RootMeanSquaredError: 0.5241 - loss: 0.2747 - val_RootMeanSquaredError: 0.5669 - val_loss: 0.3214\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - RootMeanSquaredError: 0.5216 - loss: 0.2722 - val_RootMeanSquaredError: 0.6745 - val_loss: 0.4549\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - RootMeanSquaredError: 0.5190 - loss: 0.2694 - val_RootMeanSquaredError: 0.6051 - val_loss: 0.3661\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - RootMeanSquaredError: 0.5281 - loss: 0.2790\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\n",
    "model = tf.keras.Sequential([\n",
    "    norm_layer,\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
    "norm_layer.adapt(X_train)\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    validation_data=(X_valid, y_valid)\n",
    ")\n",
    "\n",
    "mse_test, rmse_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Complex Models Using the Functional API\n",
    "\n",
    "* It connects all or part of the inputs directly to the output layer. This architecture makes it possible for the neural network to learn both deep patterns (using the deep path) and simple rules (through the short path). In contrast, a regular MLP forces all the data to flow through the full stack of layers; thus, simple patterns in the data may end up being distorted by this sequence of transformations.\n",
    "\n",
    "* Let’s build such a neural network to tackle the California housing problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Normalization()\n",
    "\n",
    "hidden_layer1 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "hidden_layer2 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "concat_layer = tf.keras.layers.Concatenate()\n",
    "output_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "input_ = tf.keras.layers.Input(shape=X_train.shape[1:])\n",
    "normalized = normalization_layer(input_)\n",
    "\n",
    "hidden1 = hidden_layer1(normalized)\n",
    "hidden2 = hidden_layer2(hidden1)\n",
    "\n",
    "concat = concat_layer([normalized, hidden2])\n",
    "\n",
    "output = output_layer(concat)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_],\n",
    "    outputs=[output]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* But what if you want to send a subset of the features through the wide path and a different subset (possibly overlapping) through the deep path ? In this case, one solution is to use multiple inputs. For example, suppose we want to send five features through the wide path (features 0 to 4), and six features through the deep path (features 2 to 7). We can do this as follows.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_wide = tf.keras.layers.Input(shape=[5]) # features 0 to 4\n",
    "input_deep = tf.keras.layers.Input(shape=[6]) # features 2 to 7\n",
    "\n",
    "norm_layer_wide = tf.keras.layers.Normalization()\n",
    "norm_layer_deep = tf.keras.layers.Normalization()\n",
    "\n",
    "norm_wide = norm_layer_wide(input_wide)\n",
    "norm_deep = norm_layer_deep(input_deep)\n",
    "\n",
    "hidden1 = tf.keras.layers.Dense(30, activation=\"relu\")(norm_deep)\n",
    "hidden2 = tf.keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "\n",
    "concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
    "output = tf.keras.layers.Dense(1)(concat)\n",
    "model = tf.keras.Model(inputs=[input_wide, input_deep], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we can compile the model as usual, but when we call the fit() method, instead of passing a single input matrix X_train, we must pass a pair of matrices (X_train_wide, X_train_deep), one per input. The same is true for X_valid, and also for X_test and X_new when you call evaluate() or predict():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - RootMeanSquaredError: 1.5356 - loss: 2.4801 - val_RootMeanSquaredError: 1.2508 - val_loss: 1.5646\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - RootMeanSquaredError: 0.7180 - loss: 0.5162 - val_RootMeanSquaredError: 0.7340 - val_loss: 0.5387\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - RootMeanSquaredError: 0.6605 - loss: 0.4364 - val_RootMeanSquaredError: 0.6120 - val_loss: 0.3745\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - RootMeanSquaredError: 0.6429 - loss: 0.4135 - val_RootMeanSquaredError: 0.6002 - val_loss: 0.3603\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - RootMeanSquaredError: 0.6317 - loss: 0.3992 - val_RootMeanSquaredError: 0.5921 - val_loss: 0.3505\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - RootMeanSquaredError: 0.6226 - loss: 0.3878 - val_RootMeanSquaredError: 0.6085 - val_loss: 0.3703\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - RootMeanSquaredError: 0.6139 - loss: 0.3769 - val_RootMeanSquaredError: 0.5771 - val_loss: 0.3330\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - RootMeanSquaredError: 0.6074 - loss: 0.3691 - val_RootMeanSquaredError: 0.6553 - val_loss: 0.4294\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - RootMeanSquaredError: 0.6022 - loss: 0.3627 - val_RootMeanSquaredError: 0.5867 - val_loss: 0.3442\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - RootMeanSquaredError: 0.5977 - loss: 0.3574 - val_RootMeanSquaredError: 0.7882 - val_loss: 0.6212\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - RootMeanSquaredError: 0.5933 - loss: 0.3521 - val_RootMeanSquaredError: 0.6857 - val_loss: 0.4702\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - RootMeanSquaredError: 0.5904 - loss: 0.3486 - val_RootMeanSquaredError: 0.9694 - val_loss: 0.9398\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - RootMeanSquaredError: 0.5879 - loss: 0.3457 - val_RootMeanSquaredError: 0.8786 - val_loss: 0.7719\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - RootMeanSquaredError: 0.5865 - loss: 0.3440 - val_RootMeanSquaredError: 0.9843 - val_loss: 0.9689\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - RootMeanSquaredError: 0.5828 - loss: 0.3397 - val_RootMeanSquaredError: 0.7505 - val_loss: 0.5633\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - RootMeanSquaredError: 0.5809 - loss: 0.3374 - val_RootMeanSquaredError: 0.7816 - val_loss: 0.6109\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - RootMeanSquaredError: 0.5787 - loss: 0.3349 - val_RootMeanSquaredError: 0.7085 - val_loss: 0.5020\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - RootMeanSquaredError: 0.5772 - loss: 0.3332 - val_RootMeanSquaredError: 0.7520 - val_loss: 0.5655\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - RootMeanSquaredError: 0.5755 - loss: 0.3312 - val_RootMeanSquaredError: 0.8040 - val_loss: 0.6464\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - RootMeanSquaredError: 0.5759 - loss: 0.3317 - val_RootMeanSquaredError: 1.0010 - val_loss: 1.0021\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - RootMeanSquaredError: 0.5757 - loss: 0.3315\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468ms/step\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
    "\n",
    "X_train_wide, X_train_deep = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_wide, X_valid_deep = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_wide, X_test_deep = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_wide, X_new_deep = X_test_wide[:3], X_test_deep[:3]\n",
    "\n",
    "norm_layer_wide.adapt(X_train_wide)\n",
    "norm_layer_deep.adapt(X_train_deep)\n",
    "\n",
    "history = model.fit((X_train_wide,X_train_deep), y_train, epochs=20, validation_data=((X_valid_wide,X_valid_deep),y_valid))\n",
    "\n",
    "mse_test = model.evaluate((X_test_wide, X_test_deep), y_test)\n",
    "y_pred = model.predict((X_new_wide, X_new_deep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adding an auxiliary output for regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\MRE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_wide = tf.keras.layers.Input(shape=[5])  # features 0 to 4\n",
    "input_deep = tf.keras.layers.Input(shape=[6])  # features 2 to 7\n",
    "\n",
    "norm_layer_wide = tf.keras.layers.Normalization()\n",
    "norm_layer_deep = tf.keras.layers.Normalization()\n",
    "\n",
    "norm_wide = norm_layer_wide(input_wide)\n",
    "norm_deep = norm_layer_deep(input_deep)\n",
    "\n",
    "hidden1 = tf.keras.layers.Dense(30, activation=\"relu\")(norm_deep)\n",
    "hidden2 = tf.keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
    "output = tf.keras.layers.Dense(1)(concat)\n",
    "aux_output = tf.keras.layers.Dense(1)(hidden2)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_wide, input_deep], \n",
    "    outputs=[output, aux_output]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each output will need its own loss function. Therefore, when we compile the model, we should pass a list of losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "model.compile(\n",
    "    loss=(\"mse\", \"mse\"),\n",
    "    loss_weights=(0.9, 0.1),\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"RootMeanSquaredError\", \"RootMeanSquaredError\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now when we train the model, we need to provide labels for each output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - dense_2_RootMeanSquaredError: 1.5746 - dense_2_loss: 2.5828 - dense_3_RootMeanSquaredError: 1.9216 - dense_3_loss: 3.8041 - loss: 2.7049 - val_dense_2_RootMeanSquaredError: 0.8520 - val_dense_2_loss: 0.7257 - val_dense_3_RootMeanSquaredError: 2.5526 - val_dense_3_loss: 6.5127 - val_loss: 1.3049\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - dense_2_RootMeanSquaredError: 0.7899 - dense_2_loss: 0.6244 - dense_3_RootMeanSquaredError: 0.8508 - dense_3_loss: 0.7242 - loss: 0.6343 - val_dense_2_RootMeanSquaredError: 0.7268 - val_dense_2_loss: 0.5282 - val_dense_3_RootMeanSquaredError: 1.6758 - val_dense_3_loss: 2.8069 - val_loss: 0.7563\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - dense_2_RootMeanSquaredError: 0.7233 - dense_2_loss: 0.5235 - dense_3_RootMeanSquaredError: 0.7801 - dense_3_loss: 0.6088 - loss: 0.5320 - val_dense_2_RootMeanSquaredError: 0.6595 - val_dense_2_loss: 0.4349 - val_dense_3_RootMeanSquaredError: 1.0888 - val_dense_3_loss: 1.1850 - val_loss: 0.5100\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - dense_2_RootMeanSquaredError: 0.6797 - dense_2_loss: 0.4623 - dense_3_RootMeanSquaredError: 0.7446 - dense_3_loss: 0.5547 - loss: 0.4715 - val_dense_2_RootMeanSquaredError: 0.6749 - val_dense_2_loss: 0.4554 - val_dense_3_RootMeanSquaredError: 0.8371 - val_dense_3_loss: 0.7006 - val_loss: 0.4800\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - dense_2_RootMeanSquaredError: 0.6511 - dense_2_loss: 0.4242 - dense_3_RootMeanSquaredError: 0.7228 - dense_3_loss: 0.5226 - loss: 0.4340 - val_dense_2_RootMeanSquaredError: 0.7397 - val_dense_2_loss: 0.5470 - val_dense_3_RootMeanSquaredError: 0.7203 - val_dense_3_loss: 0.5187 - val_loss: 0.5443\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - dense_2_RootMeanSquaredError: 0.6321 - dense_2_loss: 0.3998 - dense_3_RootMeanSquaredError: 0.7081 - dense_3_loss: 0.5016 - loss: 0.4099 - val_dense_2_RootMeanSquaredError: 0.5937 - val_dense_2_loss: 0.3524 - val_dense_3_RootMeanSquaredError: 0.6825 - val_dense_3_loss: 0.4658 - val_loss: 0.3638\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - dense_2_RootMeanSquaredError: 0.6192 - dense_2_loss: 0.3836 - dense_3_RootMeanSquaredError: 0.6953 - dense_3_loss: 0.4836 - loss: 0.3936 - val_dense_2_RootMeanSquaredError: 0.7431 - val_dense_2_loss: 0.5520 - val_dense_3_RootMeanSquaredError: 0.8094 - val_dense_3_loss: 0.6550 - val_loss: 0.5625\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - dense_2_RootMeanSquaredError: 0.6101 - dense_2_loss: 0.3724 - dense_3_RootMeanSquaredError: 0.6853 - dense_3_loss: 0.4697 - loss: 0.3821 - val_dense_2_RootMeanSquaredError: 0.7416 - val_dense_2_loss: 0.5497 - val_dense_3_RootMeanSquaredError: 0.7520 - val_dense_3_loss: 0.5654 - val_loss: 0.5515\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - dense_2_RootMeanSquaredError: 0.6047 - dense_2_loss: 0.3658 - dense_3_RootMeanSquaredError: 0.6765 - dense_3_loss: 0.4578 - loss: 0.3750 - val_dense_2_RootMeanSquaredError: 1.3204 - val_dense_2_loss: 1.7425 - val_dense_3_RootMeanSquaredError: 1.0988 - val_dense_3_loss: 1.2069 - val_loss: 1.6897\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - dense_2_RootMeanSquaredError: 0.6011 - dense_2_loss: 0.3614 - dense_3_RootMeanSquaredError: 0.6705 - dense_3_loss: 0.4497 - loss: 0.3702 - val_dense_2_RootMeanSquaredError: 1.8405 - val_dense_2_loss: 3.3857 - val_dense_3_RootMeanSquaredError: 1.8691 - val_dense_3_loss: 3.4918 - val_loss: 3.3979\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - dense_2_RootMeanSquaredError: 0.6016 - dense_2_loss: 0.3620 - dense_3_RootMeanSquaredError: 0.6667 - dense_3_loss: 0.4446 - loss: 0.3703 - val_dense_2_RootMeanSquaredError: 1.2784 - val_dense_2_loss: 1.6336 - val_dense_3_RootMeanSquaredError: 1.2448 - val_dense_3_loss: 1.5489 - val_loss: 1.6259\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - dense_2_RootMeanSquaredError: 0.5936 - dense_2_loss: 0.3524 - dense_3_RootMeanSquaredError: 0.6598 - dense_3_loss: 0.4355 - loss: 0.3607 - val_dense_2_RootMeanSquaredError: 1.2502 - val_dense_2_loss: 1.5623 - val_dense_3_RootMeanSquaredError: 1.1157 - val_dense_3_loss: 1.2444 - val_loss: 1.5312\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - dense_2_RootMeanSquaredError: 0.5898 - dense_2_loss: 0.3480 - dense_3_RootMeanSquaredError: 0.6526 - dense_3_loss: 0.4259 - loss: 0.3558 - val_dense_2_RootMeanSquaredError: 0.8591 - val_dense_2_loss: 0.7378 - val_dense_3_RootMeanSquaredError: 0.7582 - val_dense_3_loss: 0.5746 - val_loss: 0.7218\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - dense_2_RootMeanSquaredError: 0.5847 - dense_2_loss: 0.3419 - dense_3_RootMeanSquaredError: 0.6468 - dense_3_loss: 0.4184 - loss: 0.3496 - val_dense_2_RootMeanSquaredError: 0.9630 - val_dense_2_loss: 0.9270 - val_dense_3_RootMeanSquaredError: 0.8510 - val_dense_3_loss: 0.7241 - val_loss: 0.9070\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - dense_2_RootMeanSquaredError: 0.5823 - dense_2_loss: 0.3391 - dense_3_RootMeanSquaredError: 0.6419 - dense_3_loss: 0.4122 - loss: 0.3464 - val_dense_2_RootMeanSquaredError: 0.7845 - val_dense_2_loss: 0.6152 - val_dense_3_RootMeanSquaredError: 0.6963 - val_dense_3_loss: 0.4847 - val_loss: 0.6023\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - dense_2_RootMeanSquaredError: 0.5790 - dense_2_loss: 0.3353 - dense_3_RootMeanSquaredError: 0.6376 - dense_3_loss: 0.4067 - loss: 0.3424 - val_dense_2_RootMeanSquaredError: 0.9699 - val_dense_2_loss: 0.9404 - val_dense_3_RootMeanSquaredError: 0.9323 - val_dense_3_loss: 0.8690 - val_loss: 0.9336\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - dense_2_RootMeanSquaredError: 0.5782 - dense_2_loss: 0.3343 - dense_3_RootMeanSquaredError: 0.6339 - dense_3_loss: 0.4019 - loss: 0.3411 - val_dense_2_RootMeanSquaredError: 1.0539 - val_dense_2_loss: 1.1102 - val_dense_3_RootMeanSquaredError: 0.9138 - val_dense_3_loss: 0.8347 - val_loss: 1.0831\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - dense_2_RootMeanSquaredError: 0.5769 - dense_2_loss: 0.3328 - dense_3_RootMeanSquaredError: 0.6306 - dense_3_loss: 0.3978 - loss: 0.3393 - val_dense_2_RootMeanSquaredError: 1.1138 - val_dense_2_loss: 1.2400 - val_dense_3_RootMeanSquaredError: 1.1852 - val_dense_3_loss: 1.4040 - val_loss: 1.2570\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - dense_2_RootMeanSquaredError: 0.5757 - dense_2_loss: 0.3314 - dense_3_RootMeanSquaredError: 0.6291 - dense_3_loss: 0.3959 - loss: 0.3379 - val_dense_2_RootMeanSquaredError: 0.9483 - val_dense_2_loss: 0.8989 - val_dense_3_RootMeanSquaredError: 0.8829 - val_dense_3_loss: 0.7792 - val_loss: 0.8873\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - dense_2_RootMeanSquaredError: 0.5728 - dense_2_loss: 0.3281 - dense_3_RootMeanSquaredError: 0.6242 - dense_3_loss: 0.3898 - loss: 0.3343 - val_dense_2_RootMeanSquaredError: 1.0714 - val_dense_2_loss: 1.1473 - val_dense_3_RootMeanSquaredError: 0.9785 - val_dense_3_loss: 0.9572 - val_loss: 1.1288\n"
     ]
    }
   ],
   "source": [
    "norm_layer_wide.adapt(X_train_wide)\n",
    "norm_layer_deep.adapt(X_train_deep)\n",
    "history = model.fit(\n",
    "    (X_train_wide, X_train_deep),\n",
    "    (y_train, y_train),\n",
    "    epochs=20,\n",
    "    validation_data=(\n",
    "        (X_valid_wide, X_valid_deep),\n",
    "        (y_valid, y_valid)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - dense_2_RootMeanSquaredError: 0.5758 - dense_2_loss: 0.3316 - dense_3_RootMeanSquaredError: 0.6252 - dense_3_loss: 0.3909 - loss: 0.3376\n"
     ]
    }
   ],
   "source": [
    "eval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))\n",
    "\n",
    "weighted_sum_of_losses, main_loss, aux_loss, main_rmse, aux_rmse = eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The predict() method returns a tuple, and it does not have a return_dict argument to get a dictionary instead. However, you can create one using model.output_names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_tuple = model.predict((X_new_wide, X_new_deep))\n",
    "y_pred = dict(zip(model.output_names, y_pred_tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Subclassing API to Build Dynamic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(tf.keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # needed to support naming the model\n",
    "        self.norm_layer_wide = tf.keras.layers.Normalization()\n",
    "        self.norm_layer_deep = tf.keras.layers.Normalization()\n",
    "        self.hidden1 = tf.keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = tf.keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = tf.keras.layers.Dense(1)\n",
    "        self.aux_output = tf.keras.layers.Dense(1)\n",
    "    def call(self, inputs):\n",
    "        input_wide, input_deep = inputs\n",
    "        norm_wide = self.norm_layer_wide(input_wide)\n",
    "        norm_deep = self.norm_layer_deep(input_deep)\n",
    "        hidden1 = self.hidden1(norm_deep)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
    "        output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return output, aux_output\n",
    "\n",
    "model = WideAndDeepModel(30, activation=\"relu\", name=\"my_cool_model\")\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Restoring Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"my_keras_model\", save_format=\"tf\")\n",
    "# model=tf.keras.load_model(\"my_keras_model\")\n",
    "# y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Neural Network Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=8, default=2)\n",
    "    n_neurons = hp.Int(\"n_neurons\", min_value=16, max_value=256)\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2,\n",
    "    sampling=\"log\")\n",
    "    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "        model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
