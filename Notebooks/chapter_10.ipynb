{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Biological to Artificial Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron\n",
    "\n",
    "* The perceptron is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt. It is based on a slightly different artificial neuron called a threshold logic unit (TLU), or sometimes a linear threshold unit (LTU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scikit-Learn provides a Perceptron class that can be used pretty much as you would expect—for example, on the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = (iris.target == 0) # Iris setosa\n",
    "\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "X_new = [[2, 0.5], [3, 1]]\n",
    "y_pred = per_clf.predict(X_new) # predicts True and False for these 2 flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data,\n",
    "    housing.target,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full,\n",
    "    y_train_full,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_reg = MLPRegressor(\n",
    "    hidden_layer_sizes=[50, 50, 50],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    mlp_reg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = root_mean_squared_error(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5053326657967967"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "A = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(A.data, A.target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.90357396\n",
      "Validation score: 0.333333\n",
      "Iteration 2, loss = 0.90026353\n",
      "Validation score: 0.333333\n",
      "Iteration 3, loss = 0.89697773\n",
      "Validation score: 0.333333\n",
      "Iteration 4, loss = 0.89371710\n",
      "Validation score: 0.333333\n",
      "Iteration 5, loss = 0.89049354\n",
      "Validation score: 0.333333\n",
      "Iteration 6, loss = 0.88729736\n",
      "Validation score: 0.333333\n",
      "Iteration 7, loss = 0.88412733\n",
      "Validation score: 0.333333\n",
      "Iteration 8, loss = 0.88098044\n",
      "Validation score: 0.333333\n",
      "Iteration 9, loss = 0.87786561\n",
      "Validation score: 0.333333\n",
      "Iteration 10, loss = 0.87478224\n",
      "Validation score: 0.333333\n",
      "Iteration 11, loss = 0.87170958\n",
      "Validation score: 0.333333\n",
      "Iteration 12, loss = 0.86867681\n",
      "Validation score: 0.333333\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "mlp_clf = MLPClassifier(\n",
    "    hidden_layer_sizes=[5,8],\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    verbose = True,\n",
    "    alpha=0.01,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    mlp_clf\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15,  0,  0],\n",
       "       [ 7,  4,  0],\n",
       "       [ 2, 10,  0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing MLPs with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Image Classifier Using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Keras to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's look data shapes & types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For simplicity, we’ll scale the pixel intensities down to the 0–1 range by dividing them by 255.0 (this also converts them to floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = X_train/255.0, X_valid/255.0, X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With MNIST, when the label is equal to 5, it means that the image represents the handwritten digit 5. Easy. For Fashion MNIST, however, we need the list of class names to know what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For example, the first image in the training set represents an ankle boot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ankle boot'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model using the sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now let’s build the neural network! Here is a classification MLP with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=[28, 28]))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(300, activation=\"relu\")) #Hidden 1\n",
    "model.add(tf.keras.layers.Dense(100, activation=\"relu\")) #Hidden 2\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instead of adding the layers one by one as we just did, it’s often more convenient to pass a list of layers when creating the Sequential model. You can also drop the Input layer and instead specify the input_shape in the first layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model = tf.keras.Sequential(\n",
    "#    [\n",
    "#        tf.keras.layers.Input(shape=[28, 28]),\n",
    "#        tf.keras.layers.Dense(300, activation=\"relu\"), #Hidden 1\n",
    "#        tf.keras.layers.Dense(100, activation=\"relu\"), #Hidden 2\n",
    "#        tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "#    ]\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model’s summary() method displays all the model’s layers,⁠ including each layer’s name (which is automatically generated unless you set it when creating the layer), its output shape (None means the batch size can be anything), and its number of parameters. The summary ends with the total number of parameters, including trainable and non-trainable parameters. Here we only have trainable parameters.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that Dense layers often have a lot of parameters. For example, the first hidden layer has 784 × 300 connection weights, plus 300 bias terms, which adds up to 235,500 parameters! This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting, especially when you do not have a lot of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can easily get a model’s list of layers using the layers attribute, or use the get_layer() method to access a layer by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Flatten name=flatten, built=True>,\n",
       " <Dense name=dense, built=True>,\n",
       " <Dense name=dense_1, built=True>,\n",
       " <Dense name=dense_2, built=True>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense') is hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01747624,  0.01148728, -0.0216403 , ..., -0.0172466 ,\n",
       "         0.02963133, -0.06563302],\n",
       "       [-0.0037529 , -0.06391131,  0.06885923, ...,  0.06002857,\n",
       "        -0.05007673,  0.01366003],\n",
       "       [ 0.04723161, -0.04379401, -0.00961278, ...,  0.01643698,\n",
       "        -0.00366748, -0.0522002 ],\n",
       "       ...,\n",
       "       [ 0.06255904,  0.01783132, -0.06407811, ..., -0.03835736,\n",
       "        -0.02504823, -0.06765718],\n",
       "       [-0.06577045,  0.04297391,  0.00430641, ..., -0.05847385,\n",
       "        -0.00355236,  0.03368064],\n",
       "       [-0.0216355 , -0.07124308, -0.02407493, ...,  0.06376481,\n",
       "        -0.07389132,  0.01611587]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer = \"sgd\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - accuracy: 0.6836 - loss: 0.9848 - val_accuracy: 0.8224 - val_loss: 0.5076\n",
      "Epoch 2/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.8245 - loss: 0.5083 - val_accuracy: 0.8372 - val_loss: 0.4553\n",
      "Epoch 3/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.8414 - loss: 0.4550 - val_accuracy: 0.8474 - val_loss: 0.4309\n",
      "Epoch 4/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.8521 - loss: 0.4247 - val_accuracy: 0.8518 - val_loss: 0.4134\n",
      "Epoch 5/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.8593 - loss: 0.4027 - val_accuracy: 0.8566 - val_loss: 0.4022\n",
      "Epoch 6/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.8647 - loss: 0.3857 - val_accuracy: 0.8604 - val_loss: 0.3916\n",
      "Epoch 7/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.8688 - loss: 0.3713 - val_accuracy: 0.8614 - val_loss: 0.3847\n",
      "Epoch 8/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.8736 - loss: 0.3587 - val_accuracy: 0.8632 - val_loss: 0.3781\n",
      "Epoch 9/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.8774 - loss: 0.3477 - val_accuracy: 0.8648 - val_loss: 0.3728\n",
      "Epoch 10/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.8810 - loss: 0.3373 - val_accuracy: 0.8678 - val_loss: 0.3696\n",
      "Epoch 11/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.8841 - loss: 0.3281 - val_accuracy: 0.8682 - val_loss: 0.3655\n",
      "Epoch 12/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 15ms/step - accuracy: 0.8861 - loss: 0.3198 - val_accuracy: 0.8678 - val_loss: 0.3625\n",
      "Epoch 13/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 13ms/step - accuracy: 0.8889 - loss: 0.3119 - val_accuracy: 0.8690 - val_loss: 0.3599\n",
      "Epoch 14/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 12ms/step - accuracy: 0.8911 - loss: 0.3045 - val_accuracy: 0.8700 - val_loss: 0.3568\n",
      "Epoch 15/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.8937 - loss: 0.2975 - val_accuracy: 0.8706 - val_loss: 0.3551\n",
      "Epoch 16/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - accuracy: 0.8956 - loss: 0.2911 - val_accuracy: 0.8704 - val_loss: 0.3535\n",
      "Epoch 17/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.8975 - loss: 0.2850 - val_accuracy: 0.8710 - val_loss: 0.3514\n",
      "Epoch 18/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 15ms/step - accuracy: 0.9002 - loss: 0.2791 - val_accuracy: 0.8714 - val_loss: 0.3507\n",
      "Epoch 19/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 13ms/step - accuracy: 0.9026 - loss: 0.2735 - val_accuracy: 0.8710 - val_loss: 0.3512\n",
      "Epoch 20/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 13ms/step - accuracy: 0.9040 - loss: 0.2683 - val_accuracy: 0.8718 - val_loss: 0.3504\n",
      "Epoch 21/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 13ms/step - accuracy: 0.9057 - loss: 0.2634 - val_accuracy: 0.8718 - val_loss: 0.3488\n",
      "Epoch 22/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 14ms/step - accuracy: 0.9076 - loss: 0.2585 - val_accuracy: 0.8736 - val_loss: 0.3492\n",
      "Epoch 23/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.9090 - loss: 0.2539 - val_accuracy: 0.8730 - val_loss: 0.3476\n",
      "Epoch 24/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13ms/step - accuracy: 0.9105 - loss: 0.2493 - val_accuracy: 0.8728 - val_loss: 0.3478\n",
      "Epoch 25/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.9116 - loss: 0.2449 - val_accuracy: 0.8740 - val_loss: 0.3479\n",
      "Epoch 26/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 13ms/step - accuracy: 0.9138 - loss: 0.2407 - val_accuracy: 0.8740 - val_loss: 0.3471\n",
      "Epoch 27/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 16ms/step - accuracy: 0.9151 - loss: 0.2366 - val_accuracy: 0.8752 - val_loss: 0.3480\n",
      "Epoch 28/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 14ms/step - accuracy: 0.9170 - loss: 0.2325 - val_accuracy: 0.8748 - val_loss: 0.3493\n",
      "Epoch 29/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 16ms/step - accuracy: 0.9186 - loss: 0.2287 - val_accuracy: 0.8748 - val_loss: 0.3499\n",
      "Epoch 30/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 13ms/step - accuracy: 0.9207 - loss: 0.2248 - val_accuracy: 0.8754 - val_loss: 0.3499\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The fit() method returns a History object containing the training parameters (history.params), the list of epochs it went through (history.epoch), and most importantly a dictionary (history.history) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set (if any). If you use this dictionary to create a Pandas DataFrame and call its plot() method, you get the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Epoch'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAFBCAYAAABEo8fdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAABHgElEQVR4nO3deXxU1f3/8dfJzmYgJLKKgAKCLFI3EIUgWq0/646I2ooVFWzda7W477Zaa/0WUbTuK24tLtW6ENGKVlAEBUVFEBAMYYlEyH5+f3xymUnIZIGQO0nez8fjPmbmLjMnl5D3nHPPPcd57xEREZHwJIRdABERkZZOYSwiIhIyhbGIiEjIFMYiIiIhUxiLiIiETGEsIiISslrD2Dn3oHMu1zn3WYztzjl3t3Pua+fcAufczxq+mCIiIs1XXWrGDwNH1rD9F0CfiuUcYNqOF0tERKTlqDWMvfezgfU17HIs8Kg3HwDtnXNdGqqAIiIizV1DXDPuBqyIer2yYp2IiIjUQVJjfphz7hysKZu0tLR9e/To0Zgf3+SVl5eTkKA+d/Whc1Z/Omf1p3NWfy3xnC1ZsiTPe59V3baGCONVwG5Rr7tXrNuG9346MB2gX79+/ssvv2yAj285cnJyyM7ODrsYTYrOWf3pnNWfzln9tcRz5pxbHmtbQ3wtmQn8uqJX9TAg33u/ugHeV0REpEWotWbsnHsKyAYynXMrgWuBZADv/b3Aq8BRwNfAZuDMnVVYERGR5qjWMPbej69luwd+22AlEhERaWFa1tVzERGROKQwFhERCZnCWEREJGQKYxERkZApjEVEREKmMBYREQmZwlhERCRkCmMREZGQKYxFRERCpjAWEREJmcJYREQkZApjERGRkCmMRUREQqYwFhERCZnCWEREJGQKYxERkZAlhV0AERGRnaasDH76CbZssaWwEIqLoU8faNUKVq6ERYtsXfRy/PHQrh3873/wzjuR9UVF9h433wxt2sCTT8KMGbYuWIqK7LjkZJgyBaZNs/U1UBiLiEjD8B5KSy14kpIs7IqK4KuvKodVYSFpBQV2TF4ezJwJJSWVl6OPhgED4Jtv4N57I+uDULz4YthnH3jvPbj88kjYBss//wnDh1tY/vrX25b1449h6FB4+WWYPHnb7cOHWxi/8w784Q+R9Skp9nNdcYWF8fr18O23kJZmS3q6PZaWWhj/7Gf2+ampcPvtMU+dwlhEpCkqL4eEiiuNubmRGlkQVu3awZ572vbXX7faYXTtrk8fyM627TfcUPn4oiI49FA45RRbf+KJkfXBPhMnwm9/Cz/8YKEZBG15ub3n7bfD738P330HgwZtU/yMiy6y91+xAs46a9ufr2tXe9/Vq+GeeyzYgiUlBU4/3fYLQj8jwx6DJSPDtu+3n5UleltqKvTsaduPPRYGD7b3jF522822X3ABnHeerUtKAucql/N3v7MllpNOsiU4JzEojEVE6sp7C6LNm+2xUycAWq1caTW0IKwKCy00fvlLO+6FF+DrryPbi4qgY0erXQFcdRV8/nnl7f37w/332/bDD4eFCytvP/xweO01277ffhZq0U48EZ57zp6PHw8bNlTefsYZkTC+6Sb72VJTLXRSU6FzZ9uWmGiBG6zv0MGet29v29u2hVNPjdQMg2XkSNvetSs8+2xkfWoqpKay9vvv6QsWuMuWbRu2qal2/MEH2xeJWIYNgzffjL29f39bYunSxZZYKsq7symMRaRp8d7CaPNmaxJMTIQ1a6ypMDqsioosDNPS4MMPYc6cyjW7oiK45Rb74//44/DSS9a8uXmzPZaU2HU/gPPPh4cesm3e27r27bcGXK8HHrDmzGjdu0fC+P77I8GZkGB/3AcNioTxt99ac2xqaiS02rSJvNdBB0Hv3pFgSE2Ffv0i22+7zb4ABCGWkmKfH3jrrcjnBjW/du0i2wsLI7XsqpKTYe7c2P8ebdrA//1fzduDmmGUkpwce5KaCrvvHvv4FkJhLCINI7hWGITZ5s3QrRvssovVrD74wNZFbx83zv4Qf/QR3HdfZH2w3H8/7LWXheXFF0eODQJxyRJrbn38cbjssm3LtGqV1cz+/W+4/vrI+sREC6Srr7aw+e47+PRTaN3amjGDx6ApeMQI2z9Y36pVpTD77rTT2PXKKyvV/GjdOvJ5zz5rzZupqdbUWdUTT9R8bqPLXp1TT615+9ChNW+PFcTSaBTGIs1R0JzqfaQzyRdfVK75bdliQde/P2zcuDUMe3/5pTWrbtli1/TGjIGlS+0aYWFh5V6pd9xhtZ733oNDDtm2HC+8YL1SP/kEjjtu2+377GNhvGYNvPqq1aJat44swfXHXr3g5JMrb2vd2pp6wZpkBw6sXHNMTYWsLNt+2WVw0UWRmmNiYuVyTJliSyynnGJLDAXR11+r07Zt7G0iKIxFwuV9pEPIihVQUGBh+dNP9jwrC/bf37b/6U+Qnw+bNtm2ggLrZDN5sjWp9u8fWR80p/7hD3bcpk3VdqLh+uvhmmvsmIom0+7JyZEa4LBhtl9Cgn1G27aQmRmpHQbXFXv2tE5A0bXKVq3sWiZYz9R58yIhGr0fWHNu0KRbnREjbImlVy9bYolu8hWJQwpjkZqUlUVqUStWwLp1FpTBkpYGv/iFbX/gAbvuF4Tp5s12ne+mm2z78cfD4sWRJtiffrIwfeUV2z58uDWrRhs71u5hBLj1VjumXTsLxbZtIwGblGTHt25twdOmjQXd8OG2vV07e5/ooGzd2ppwwR4LCqBVK2bPnk121Vpez57w7ruxz1P37tbkG0t6ut3iISLVUhhL01ZaamHpnN0C8f33kdphQQGdP/kk0nz44ovWiSf6mmRiYuR63aWX2v2O0ds7d4bly237xInwn/9U/vwBAyJh/PDD1uEnaGpt06bytbgePSLXEoNlwIDI9r/+1cI/2Nau3dbeuoDdvpKSUv15cA4eeyz2eUpKsmCPJSFBtUeRECmMJRylpdbkunGj1apSU63W+P77ti56uftuu53ivvus12YQtj/9ZNctN22yWuLtt1ugRdkLrMdsQoL1Zn300crXJYNrjmD3FR5wQOWwDK45gl1TnDQpUvNs3TpyewdYb9qq1yKj/e1vNZ+TmsISYgexiDR5CmPZPiUlFqb5+bDrrlaL++47eOMNC9AgaDdutHso+/a1zjwXXWS3gwSj74DdPzlwoN1+cf75ti4hwYIuPd1Ct0MHW/r1izTRtmljj0Htc8IEqwUH29u25YMFCxgWXJO9914L9Fguuqjmn3nUqJq31xTEIiI1UBi3dEVFkR63+fkWpuvWVV4mTrQb7z/80HqtbtxY+Sb8f/0LjjkGPvvM9gVrNg3CdN06W9elCxx2mK2PXoIb7k8/3UbDad/ewrTqSDcnn2xLLIMH2xKlMDc38j5V30+kgXlv31OrjPxYaQk6ogdLcbF9n0xMjCzRr2vaFlyhCRao/LqmJbq8xcU1P8ZatyPnaenSXrz6at0/K3pdWZmdB+fssepS3frodTvyp6C83D6/rKzy89peBzcGxKIwbk68t1tE8vJg7Vp7zMuDIUOsJ+ratRZ4eXmRoC0osKbdiy6y663RTaVBM+5RR9nrrCw44ohtwzS4h3HUKLu+GoRp1XsXhw+PdCiqTvB+TUhZWc1/wEpLt/+xvv/Rq/6nr+6Pb13+SC9f3ptXXqn9c2K9Li+3X8XgefRS3frodcEfzNr+mFa3Huy8lZXZY/Tz6tZV3b69vIfCwhFbb7Ou7Y+umISEHqSmRgbcih58q7p1bdpEnicmRn5v6vK7Fvx/8n7H/q3Bft+CL0JJSZHy1OXL0yOPxH5fhXG88t5qqnl59q8Y3LZxxx3WkSc6bI8+Gq680mq5Qe/YaJddZmGclma12s6dYe+9LWgzM210H7Cev59+aus7drT9o/XuDf/4R+wyB9dSqygutsu6P/4Y+zHWtk2bKv+Rr+sf9ch/whGkpNTvj3qwQOxv6sFjMPZEGGLVooKyex9Zqr6uaYHuJCXVvWZW3efXdn6TkmLXVmL9ewbBGesPr/f2vkHZk5LsVzh4Hr0++nl0DXN75eb+wB57dK80GmSrVtuOEFl1adXKAib4+erzBSxYtu/fOHJXXdXwqykYox939Jzl5Lyzba/9Zk5hHE8+/9xuXwmCdO1aa6Y97zzbfuihNp3XunX2lwdsUIVnn7Xnwe0tWVkWpFlZ1hQM9r97+nTo0IGyDpmUtM+itH0mJe0yKMmFoqJ2bHnsw22bztZA4QwoLEylsHDwNs1o0bOCBcPuVvc81ra61hTatLFLz7vsEnns0cP+89c3SIN1q1bl0rVrt+0Kc++3749U9BC7ycn2B7/q81iPwfMgJGI1V+7MFvecnGpubZIa5eR8TXZ299p3FIlBYdzQPv3UAvfbb23w82+/tabXYMD2CRNg7lw8sIVWbCCDDSOOZsMg69e0PmkiG3olsWHPTDa4DmwoT2fD9xlsOAh++GE/UnddS2mZo6TEUZoHJauh9GMouTJo4jyb0tKGq7ElJ28zvvs2z9u2rX5b8DoYOTA6ZKs+tm27c/o/5eR8RXZ2t4Z/YxGRBqQwrq8vv7TAjQ7bzZspfXs269bB2kseJPftheSyK7lt92DtLsPJTe9D7vHWurx+7TtsyEhiw6Ykiksq2hP/C4wMPsDGmA36PwWdiDt0gK5dt9ClS9taa1fRj9HPq2s2q6kprbpRA0VEpOEpjKuTn0/R/MWsm/stefNXkrdkPXkX3kjexiTyHv2G3A9zyaUXuUkHszapM7nlWaxL8XjvgKh7SQsgYTNklUHW19aiPHD/1pUCNiOjcuAGS3r6tv2fcnI+V/OhiEgz1KLCeNOmKpdsVxWR99V68pb9RF6bHuTlp5C3NJ916x2bGAYMixx8WvDkKDLSS9m1k2PXzon0z4JRu9qttllZ9hj9PCNDE6KIiEjNmmUYl5fbEMELFlReli6tumcq7WhDJlvI7LeFrN4p9O9USubGb8js1Y7MvTrSsW8mmbsmkJlp/aUyMiA5uVmeNhERCUmTT5UNG2wAp+jQXbjQhhUGSHDl9Gm9in0Hw5ln7sagDivpdffFZA7YlY5De5A6ZC+7zadXW0gE6FixiIiINI4mFcY//WTTps6ebX2oFiywiXQCGRkwZFA5Z/d4g8GrX2dw/mwG+EW03m13+O1VcNppQHf47bOh/QwiIiJVxXUYl5TYCIxvv23DFs+ZY+uSkmzq1pEHlTK49dcM3jibwV3W0uXvV+JcAhx0PeyzOxx+Hhx+uE0AICIiEqfiKozLy622+9ZbtsyebbVh52wq1IsvhjGjSjn447tp/dZL8OL7NhRSaqqNaRwMhPD++6H+HCIiIvURahh7bx2tgvCdNct6OYNNznPGGTBmjE3Ek5ERHJUEX2AXiy+4wGq+hxxiN8yKiIg0QaGF8Zo1afTsabPuAXTrZvMRjBljI0J2r25kudxcu1/o4ovhkksas7giIiI7TWhhXFCQxGGHweWXWwD37VvLeLuPP27jN7/zTmSWIBERkWYgtDDec88Cnn++jju//z6cdZZNv7f33ju1XCIiIo0t/seGWrYMjjvOpu95/nmbFkdERKQZqVMYO+eOdM596Zz72jl3RTXbezjnZjnnPnHOLXDOHdUgpfvxR5urt6QEXn7Z5tgVERFpZmoNY+dcIjAV+AUwABjvnBtQZbergBne+6HAKcA9DVK6tDQYMcLm8u3Xr0HeUkREJN7U5ZrxAcDX3vulAM65p4FjgUVR+3hgl4rn6cD3O1yyn36y2ebvu2+H30pERCSeOV/LLPTOuZOAI733Eyte/wo40Hv/u6h9ugD/AToAbYDDvPfzqnmvc4BzALKysvadMWNGtZ/ZZeZMejzzDJ/cdRfFWVnb9YM1RwUFBbRt2zbsYjQpOmf1p3NWfzpn9dcSz9no0aPnee/3q25bQ/WmHg887L3/i3NuOPCYc26g9748eifv/XRgOkC/fv18tXPzvvUW3H03/PznHHTCCZrdPkpOTo7mM64nnbP60zmrP52z+tM5q6wuHbhWAdGDO3evWBftLGAGgPd+DpAGZNa7NF9+CSedBHvtBU8/rSAWEZEWoS5h/BHQxznXyzmXgnXQmllln++AMQDOuf5YGK+tV0nWrbOe08nJ1nN6l11qP0ZERKQZqDWMvfelwO+A14HFWK/pz51zNzjnjqnY7VLgbOfcp8BTwARf28XoqsrLYffd4Z//hJ4963WoiIhIU1ana8be+1eBV6usuybq+SJgxHaVwHsoK4OsLHjjjVrGxBQREWl+wh+B68474YgjoKBAQSwiIi1SuGH80ktw2WXQoQO0bh1qUURERMISWhgnFBXBqafCz34Gjz4KCeFX0kVERMIQWgK2XrUK0tNh5kzVikVEpEULLYx9QoIFcdeuYRVBREQkLoQWxj/17GlN1CIiIi2cLtSKiIiETGEsIiISMoWxiIhIyBTGIiIiIVMYi4iIhExhLCIiEjKFsYiISMgUxiIiIiFTGIuIiIRMYSwiIhKy0MI4Zf16mDMnrI8XERGJG6GFcWpeHowZo0AWEZEWL9xm6uJiyMkJtQgiIiJhCzeMk5IgOzvUIoiIiIQt3DA++WQYPjzUIoiIiIQttDAuS0uDr74K6+NFRETiRnhh3Lo1fPQRFBSEVQQREZG4EFoYl6Snw7x50Lp1WEUQERGJC0lhfXB5cjIMGRLWx4uIiMSNcDtwvfUW3HxzqEUQEREJW7hhnJMD11wD+fmhFkNERCRM4Ybx6NFQXg7vvRdqMURERMIUbhgPHw4pKTBrVqjFEBERCVO4YdyqFQwbpjAWEZEWLfwpFEePhg0boKgo7JKIiIiEIvwwvvJKWLoUUlPDLomIiEgowg/j5OSwSyAiIhKq8MMY4JZb4Mgjwy6FiIhIKOIjjMvK4D//gfXrwy6JiIhIo4uPMB49GryH2bPDLomIiEiji48wPuAAu81JtziJiEgLFB9hnJICI0YojEVEpEUKbdambZx6Knz6qQ2PmRAf3xFEREQaQ/yE8Zlnhl0CERGRUMRXFbS0FFasCLsUIiIijSq+wvikk3S/sYiItDjxFcbDhsGiRZCbG3ZJREREGk18hfHo0faYkxNqMURERBpTfIXxvvtC27YKYxERaVHqFMbOuSOdc1865752zl0RY5+TnXOLnHOfO+ee3K7SJCXBIYfofmMREWlRar21yTmXCEwFDgdWAh8552Z67xdF7dMH+CMwwnu/wTm363aXaMoUzW0sIiItSl3uMz4A+Np7vxTAOfc0cCywKGqfs4Gp3vsNAN777e+BdfDB232oiIhIU1SXZupuQPTNvysr1kXrC/R1zv3XOfeBc27H7k96913417926C1ERESaioYagSsJ6ANkA92B2c65Qd77jdE7OefOAc4ByMrKIidGR62BU6bQesUK/pee3kDFax4KCgpinjOpns5Z/emc1Z/OWf3pnFVWlzBeBewW9bp7xbpoK4EPvfclwLfOuSVYOH8UvZP3fjowHaBfv34+Ozu7+k886SS49FKy+/SBblUr4S1XTk4OMc+ZVEvnrP50zupP56z+dM4qq0sz9UdAH+dcL+dcCnAKMLPKPv/EasU45zKxZuul210q3W8sIiItSK1h7L0vBX4HvA4sBmZ47z93zt3gnDumYrfXgXXOuUXALOAy7/267S7V4MHQvr1ucRIRkRahTteMvfevAq9WWXdN1HMPXFKx7LjERBg1CubObZC3ExERiWfxM4ViVfffDx06hF0KERGRnS5+wzgrK+wSiIiINIr4Gpu6qmuugeuvD7sUIiIiO1V8h/Fnn8HDD4ddChERkZ0qvsN49GhYtswWERGRZir+wxh0v7GIiDRr8R3GAwZAZqbuNxYRkWYtfntTAyQkwNixkJoadklERER2mvgOY4B77gm7BCIiIjtVfDdTRyssDLsEIiIiO0XTCOMDD4SJE8MuhYiIyE7RNMK4Z0/rxOV92CURERFpcE0jjEePhu+/h6+/DrskIiIiDa7phDHoFicREWmWmkYY9+0LnTsrjEVEpFmK/1ubAJyDG2+EXXcNuyQiIiINrmmEMag3tYiINFuhNVOvWNGaNWvqcYD3sGABLFy408okIiIShtDCeMuWRG64oZ4HHXUU3HTTTimPiIhIWELtwDVtml0ObtWqDjs7Z72qc3J0v7GIiDQroYaxc3DCCfDtt3U8YPRoyM2FxYt3arlEREQaU6hh7D28+WY9hp3OzrZH3eIkIiLNSGhhvPvuP3HCCbBlC4wYAZ99VoeDevWCHj2sqVpERKSZCC2MU1PLef55+PhjqyGPHAlz5tRyUHC/8YABddhZRESkaQh9BK6BA+G//4WOHeGww+D112vYec4cmDQJbr4ZDj1UgSwiIs1C6GEM1vr83nvQpw/88pfwzDMxdszJgeJiKCuzC82XXAIlJY1ZVBERkQYXF2EM0KkTvPMODBsG48fbbU/byM6GlBRITISkJPjgAxgzBn74obGLKyIi0mDiJowB0tOtmfroo+G882x8j0q3FA8fDm+9ZdeNZ8+GJ5+EuXNh333hf/8LrdwiIiI7Iu7Gpm7VCp5/Hs46C66+GvLy4M47ISH42jB8uC3B8wED4Ljj4J//hAMOCKnUIiIi2y/uwhggORkeftg6dd11F6xbBw8+aOu3MWSIdcneZRd7vWQJ9OxpzdkiIiJNQFyGMVhN+M47ITMTrroKNm6EGTNiDJ3ZoYM9FhTYKF09e8Jzz0GXLo1YYhERke0TV9eMq3IOrrzSOnO98goccQR88QWMGkX1Mz61bWsJPn8+7LefdfASERGJc3EdxoFJk+CppyxbDz4Y3n2X2DM+jRtn9x+nptpIIvff36hlFRERqa8mEcYAEybYLcXr1lkP6xpnfBo82HpZjx5tHbvKyxu5tCIiInXXZMJ46VI49VSr8AZat7ZxP9atq+aAjAx49VUbQSQhAVatgtWrG628IiIiddVkwrhLF+swXVICaWlWK+7QAW65Bbp3h7PPhgULqhyUmGjXkQF+/Wu7H3n6dLj1Vg2lKSIicaPJhDHYQFuTJtm148mT7bbihQstZ594wu5yGj0aXnzRRsys5K67rIZ87rnWK2zMGAWyiIjEhSYVxi+8AFOnWuhOnWqvBw6E++6DlSvhz3+25uwTToA99oA77oANGyoOHjQIfvMbe+69zd34+OOh/SwiIiKBJhXGNcnIgMsug2++sRG8eva01927W2160SLgF79gdVovRvEOaxK6wtixdvD//gdr14ZZfBERacGaTRgHkpKsZpyTY7cbn3KKjea1995w+DXDOWufebznDuaGY+fZxBPl5TYzRc+e8Pvfx7iBWUREZOdpdmEcbcgQ+Mc/rAk7KQnefBP+/UEHyn0C017sbLdGtUmAl1+2BP/rX20+xwsusINEREQaQbMO40BmJnz3ndWSqw5Z7RyccGV/HjnsMdZ98JXdPzVtmt2nLCIi0ghaRBiD3RrVvj2UltqtUQkJNlXjhAl2yXjCBNh1WG+yv/kHf52Sy9KBx9iBN94IZ54JX33F6tU1DMUpIiKynVpMGEPlW6MmTbJZoO65B1asgI8+gilTYP16uOSGDuzRJ4FBg+Cq/4xk7pNL8P324sYDZvLe7HJuOE9pLCIiDSduZ23aGV54IfJ86tTIc+dsXon99rOK8NKlMHOmjaR567ujuLn8v7ZjxWXkaS92ZpqzGvaWLY1WfBERaabqVDN2zh3pnPvSOfe1c+6KGvY70TnnnXP7NVwRG1/v3nDRRdYjOzcX/vY36J6eD/it+7RKLmbU0B+5se9jvPKHd1izdHOt76tmbhERqU6tYeycSwSmAr8ABgDjnXMDqtmvHXAh8GFDFzJMHTta5+qjR28hgXJSKMJRTrdOZSxfkcC1X53G0bePosserenWegPHDF/L9deW89JL8P33ld/rxhvhvfdqmHFKRERapLo0Ux8AfO29XwrgnHsaOBZYVGW/G4E/AZc1aAnjxA+uM5NOWMM53V5h+qr/x2rfmRdegE355cx/dD7znvyCefMc8z4cwssfZuIrKtGdO3tyc12liaOmTbNFzdwiIgJ1C+NuwIqo1yuBA6N3cM79DNjNe/+Kc65ZhrFdb+4MnEXU5WbapSdwyPn7cMj5+0BhISxcSEF/x/z58PHJtzFvc38+yDiYJXkZgKs4ytOpk+O44+Cxx2CffWCvvaxDWSyrV8OFF+7D669D58474ycUEZGwOO99zTs4dxJwpPd+YsXrXwEHeu9/V/E6AXgbmOC9X+acywF+773f5kZd59w5wDkAWVlZ+86YMaMhf5b4UlZG11deodMbb5D+2WdMZDoPchaJlFJKMh3abuGn4lSKixMBSE4up2fPn9hjjwL23NOWPfYooG1bm/Hir3/tw0svdeWXv/yeiy/+KsyfrEkpKCigbTBzl9SJzln96ZzVX0s8Z6NHj57nva+2T1Vdwng4cJ33/oiK138E8N7fWvE6HfgGKKg4pDOwHjimukAO9OvXz3/55Zf1/FGaqKVLOWHYKrqsXcA5TGc657K6zyHMGDOdrwafyPy0Ycz/Io358+GTTyoPk+0cVPdPVJ8m7tWrbcCTZ55pebXqnJwcsrOzwy5Gk6JzVn86Z/XXEs+Zcy5mGNelmfojoI9zrhewCjgFODXY6L3PBzKjPiyHGDXjFqt3b1741w8w5jIoLmZqyu/hnBvg2gfpv/nv9E9NZfzIkfCLX+CfmsCaog7Mn29ja8+ZY726N22q/JbeQ79+sPvulZcePeyxe3cbAhQqdxy7555G/tlFRKRWtYax977UOfc74HUgEXjQe/+5c+4GYK73fubOLmSzMHw4vPWWJWt2tr0+/3x4913497/h1Vfh0ktxv/oVXbpAly9m8YuBBXDBoUz+fRum3+dJdKWU+iQOOsgxbBgsX27Lp5/aLVjREhKo1GkMIh3HUlKs9r3LLnUrekuuWYuINIY6DfrhvX8VeLXKumti7Ju948VqpoYPtyWQmgqHHWbLX/5iqZdZ0cjw17/CSy9BSgo/tP43k/iSs8vv4/7EyaxOOok77uhY6a23bLGRxIKAXr4cvvgCZs+24I1u6i4uhvR0W3bbzWrTwRL9uls361S2IzVrBbmISO1a1Ahcca9Ll8jzZ5+1BPz3v3nhsfHgreo7ld/CEevhiR7Qty8MHQpJSbRqZS/79q38lpMnw/TplvtFRXDccTBunAX3d99Flg8/hHXrai5eULNOTrbe5Z07W5F33TV2T/AdbSJXmItIS6AwjlepqTBmjC0nngiHHoovLsalpFjt+uc/h5ISaNMGDjoIRo6EY4+FQYMqvU0wHvc551gor15tYVydzZsrh/Tnn8O//gXLllVu8i4pgV/+svKxmZkWlsHy1FNQVhbZHgR5aqqN/92qlXVOq82O1sp1O5iINAUK46Zg+HB4+22+ffBBev/mN/Z6+XK73jx7ti1XX21drAcNsnbpu+6CkSN54ZGD4LPP4NUcpp6eXbmZvIrWra1TWL9+kXVbtliIp6VZ8/bEiXDllTak55o1FnjB82B57z1ITKwcxoGiIvv+kJRk16zT0yOP0c/vuy92mOfmQrt2tYf5jTfCwoXpal4XkbinMG4qhg/nu6Iiegdh2qULnHyyLWDVzcCCBfCnP8Ett1ROrNRUePvtGgO5qupq1sE15Zp4D2edBQ8/bE3YJSVw6KFWyf/xR8jPjzwGz1esiKyLpajIwjopCTIybOnYsfLj3XfbVJnGVQryH36Atm3ty0JN1LwuIo1JYdxcZGREno8ZAxs32lyRt9wCs2bZ+pIS6829bBk88QQceCAccADsv3/l46PEmumqNs5ZESZPrhzkkyfX7Xjv7bh//MN6fxcXwxFH2HeP9evt+nb044oVdivY+vXRQVxZUZHNaQ1WO99lF1vatYs8f+ml6mvkycnw4ot2XNu22z6mpFT+rDA7vemLgEjTozBurtq2tV7abdpYOBcXW2JkZ8OXX8K339rtVEE36z33tBp1q1b217xDB2ubhsjNzsEtWXW0vUEOFubr1m0b5meeWfuxRUVw9tnw+OOQmFhOWVkChx1mQf7jj7Zs2rTt82++sQaH3Fw7XdFKSuDoo2N/ZlKSnfKNGyuvD8I8MRH+8IfKTfOxnu9orTys6+xhfonQFxBp6modgWtnaVEjcDWQ7R6xJlaY/vgjzJtnXamXL7fUAOsI9u9/w5Ah1h798stW3UxNtXul6xHIYTnhBAvWoUM/4pNP9mf16spfDmoS9EAPauSnngpXXAE//QQFBfYY/Tz6ce1aeP99WLnSOr05Z9fiU1Is9GPV2mvinHWaT0uzf4JYj1OnVn+dPjnZGkJqOjZ4vPxy+Mc/PJMmuXoH+Xnn2bX+c8/dvi8RO3L8jn72jn4ROPLIjbz+evsW9QVG56z+x3btOqDA+0XtqtuuMG5CGm34uNdes/D+3//gv/+NVBMTE63aVVpqNechQ2DwYKvSxantOWdBkEfXyOsa5LBtmAcB4b3NJRJcI4++dv7jj9aD/bnnYNEiO8WJiXbf94ABFshFRXZ89GP08y1b7LGh7bGHfaFo1coeo5+3amWXEqr7EpCUBH//u/0c0UtSUuXXJ51kLQ9VJSfbHX5lZfbFprrHc8+NfexDD9lnBZ9X0+Mdd8Dzz9sf2ttuq/wFJTW15s6C550H997b8r7A7OhnN4Vz5n3kd62szKbTffBBa6G76676fe5FF8E//rEf3s+t9rdJYdyEhDKW63vvweGH21+8lBR48037i7UiaiKvXr3sN/z3v7fXK1bYeJzObXcTd0MJ45ztSJjHCvK6mjQJ7r8/cvxpp8G111pgVw3y6MfcXPu2v3Ch/VMnJUHv3rDvvvbPuHmzhX11j0HLQHOWkrJtS8I331Q/brxz1g2jvDzyxzz6uff2hSvWsaNHR74EBEv0F4PUVOufWV0rS1KSbfO+8udFL9deG/vYK6+050HZossYPL/tttjH33Zb9V92gucTJsT+8nT//fa+sZbrrqv+cxMTrfWquNiWkpLI8+jXCxZUf87B6hPRX/KCpeHjMXYY65qx1Ozgg60HdnSgLl9uCTN/vo3FOX9+ZGzNtWutabt9e+jZ026rKivbrp7cTdWOXCuvrvd6feTmbnv8HnvU7dhvv7V/ypSUMkpLExkzpu5fBKp+ifj1r+0Pc/QftuiltLTy61tvtXvag573xx8PU6bYsK4JCfYHN9bjlCnWDB/9BeS66yp/TqzHH36wcs+ZY19MUlLsksCxx0YGyonVItGnj/36r1ljf8QTEmwAnH79IvfRB+UPngePvXpZOKxaZeVITLRje/e29/7xx8jnVP3coqJth7oNlJbCpZfW7d+sumOvv37b9dGtArEmrgmOD76P11dJiQV1faWmWiPde+/Z705KSuWlVSsL2pQUa2VauNAuIQXnvHdv+xPXrl2kpSb4vYp+vXmzNRguWmRlTU6GgQOtH0ltwwr/+KNd6fvss+q/iGzlvQ9l6du3r5f6mTVrVthFqN3Gjd5Pm+b9ued6v9tukS/kCQne33KL94sWeX/ccd5ffbX3zzxjr0tKdlpxmsQ5ixPHH+/9eed5f//9//PnnWev63vs/Pm+3sfu6PE7+tmTJtmvZ1qaPU6eXP9jU1JKt/vY7fnckhLvzzrLjktNtcff/Mb7DRvsv2B+vvebNnlfUOD9Tz95v3mz91u2eF9Y6H1RkffnnFP5sydN8r683JbtKfs559jn5Od7v36997m53n//vfcrVnj/7bfef/WV91984f1nn3l/0kneO+d9cnKZd877U07xfskS75cu9f677+y43Fx7n/x8e9+iIu/LynbsnO3oOW+IY+Fn5T5GJqpmLA0rPd2qZmDVjao9udeuhcWLYebMyNf7lBSrNY8YAV9/DUuW2OAl3bvb7VkhNnO3JEGNPifnJyZO3L5jof6tATt6/I5+9o60RgTHDh368daOgo3xuUlJdhtf1eODW/dqs3bttsfWZUS8msreunXdji0rs5aUoUPnbT1nffps/+fWR0P8W+/Isffc8+XiWPvomnET0iTn/4x1zbiw0EL5s8+s7ej3v7d2uj/9yboug92WtWWL1a3T0qwn96BB1vZU26gdFZrkOQuZzln96ZzVX0s8Zzs6n7HI9qs6U1UgLc0uzg0dWnn95MlWQ/7sMxu+68MPbX1xsYX6U0/Z19I+fWCvvSLLqadu+9V+zhx6BPf0qFYtInFMYSzxZZddrEfFwQfbrVNVm7l//NGC/IsvrPfMCy9Y1+XTTrPjJ02ybq7p6TBzJr1KS230jxbSeUxEmiaFscSv4cOtabpqM/cRR0T2KSqy7qyBjAwbyGT2bCgpwYE1iefk2PGXXmrdI/fc02rXffrYxM0JCY32Y4mIVKUwlvgWq5k7kJoKu+8eeX3LLba8/z6MGYMvKrJpJ4NrUx98YGEdPTrGKadY8zfYPTHdulltfOVKu3dhxIiG/qlERCpRGEvzdNBB2047CTaiWHm5Be1XX9kSTEH10092w2v0wNS33WZN39OmWQ37vvvsxt099rD7qFu1avQfTUSaH4WxNF9Vp50MJCRE5oEcMyayvk0bu7v/j3+Ev/wlMrj0li22felSG9MuWrdutu+4cZCXB//5j40ksG6dXdMePVrXqkWkVgpjkWiJiTb809//Huk4du65tq1/f7thcOlSW775xh67d7ft8+ZFOpIFnLNa9bnn2shl8+bZ8Es9e9rQQSIiKIxFthWr45hzdi/0rrvCsGHbHpedbePl3XYbPPZYZDDgL76w7W++SaXRNNLTLZSfe846lC1aZIOe/PijPR5xhGrVIi2EwlikOrV1HKtOaqrVnidNsumGgpr1ySfb9nHj7L7qZctsIOjgMaghz5hReXDg66+3kJ4714L7/fdtaqfdd7cm9s6dqx/8JOTJOUSk/hTGIg0tVs26bVv42c9sqc5FF9lMD/fdF7lenZJio9iDzVX44IOR/ZOSrCPZ4sW274svWm/xv/3NRqRvQvNPi7R0CmORnWF7atbt28OvfmUjjwW16gceiNwDfdddcMklVjtevtwet2yJjDx2333w+uuR99uyxeZzDAbRvfNOawLv3t2msAmWIOxBo5aJhERhLBJPYtWqwUJz771tqc7LL9tyyilWM05MtOeBf/0L3n238hx4o0bZZ4HNG/jKK/QqK4NHHrHbvI47ru5zMIrIdlMYi8Sb7alVgzVbH3cczJpVfZi/847VuL//3u6zXrGicq144UIoK7NRy0pKbPKO+fMjndEGD7bae7dukWXECDjwQNs+e7Zd19a1apF6UxiLNDc1hXlKivXg7tlz221PPAFjxlBeVERCSopdew7ep6TEwnjVKrs9a+ZMawa/4goL4zfeiAxT6hwccAAMGADjx8Phh9uAKQsW2DjinTvb7OxVqeOZtGAKYxExFU3ky6qOWgYW4k88EXntPWzcGGny/u9/LYSD27lWrrTlwANt+9dfR547B1lZFsw33wz/7//BSy/BiSfaZLfJyfDkk7Y+NbVRfnSRsCmMRSQi1qhlVTlXedCSI4+E22+PdDx79tnKYd6jhwXu6tXWTB4sQTP5889b7Rts3PATT7Tn//63vfcHH9hALEHNOnjcf397D9WqpYlTGIvIjqup4xnY1JhHHx37+IkT4ZlnIh3PLrkEWre2+7bBRj57/30L88LCyHHz59sQpqNG2bEJCfbZ/ftDp042S1eHDhb8mzbZuvT0aue+VphLmBTGItIwtrfjGdj81W+/HTsQjz3WFu/t9qw1ayyY+/Sxa9tlZbZfebkNU/rNN3bPdjCW+D33WJM4WNP3rrtazfqddyzQR4+2Wn1yMtxxh70ORlsTaQQKYxGJD3UJc+esZpueDv362brsbAvYoIn8hRfsfcrKIvdon3pqZGzxNWvscd06SEuzLwDBlJrFxXDBBfa8bVurTYNNHvK//0UCOivLRkL71a9gzhx6PvAAlJbaxCNVa90idaAwFpGmLVYTefRQoQMG2FKd7GybCjO6ZpyVZc3fgbQ0C+x586zGnZ9vXwb23BPGjGH3LVusg1tystW4s7Ksw9o999jxjz9uYR0EeRDq8+ereVwAhbGINAc70kRe2/VugGuvtSVQVGTN5Q88AMXFdm+2czaBSO/esHZt5eOvv956lEc78EC73au42JrfDzzQmt2zsiAzE/bbDw47zPb95hu79t2+faS2D7rW3YzEVRiXlJSwcuVKCqM7aMhW6enpLF68uNE+Ly0tje7du5Nc3T2hIs1JfcM8NdVCMzsbUlLs3uzUVPjTn6p/n48/toDOzY08vvOOTQISXO9evtzu41671u7hPussC2PvYa+9rGadmAgdO9pnjxkD999vYe4c/Pa31rs8M9OWnj1t31gU5HElrsJ45cqVtGvXjp49e+J03WUbmzZtol30iEk7kfeedevWsXLlSnr16tUonynS5NR0b3a0du1s6d07sm7AAJs+M7jW/dxzkeM3b47c6lVeDg89BHl5FtTB48qVdmwQ5n/7W+XPvOkmuPJK22/48EhIZ2bae7/8sgV8SooNfTpypG3r2NF6stdGYd6g4iqMCwsLFcRxwjlHx44dWVu1uU1EKqvrvdnVHBezeTw6DBMT4fTTtz1+zhy7Dzu641qvXhbW69ZB3762X0KCjYKWl2fL8uU2FGrQPF5UFOl1HmjVyq6BH3+8NaXfckskqDMzYf16m7e7pMQ+e+ZMdV7bQXEVxoCCOI7o30JkJ9sZ17qDXuaBrl0rT70JFuRjxkSC/C9/sYFUgsDOy7POaWDB+8kntm7DhsioawkJVmsvKrKwT06GjAwL7I4d7Xp6377WFP/225W3rVhBj7ff1uxgUeIujEVEpI62N8zr0mktkJ0NX35pz8vKLJDfeMOuaQc90M86y5rh162LLMFQprNnw+WXb/O2vRIS4Kmn7NiXXqoc1hkZ1qu9dWv49FNrag/WL11qvdoPPbRZBbnCOCSlpaUkJen0i0hItifIExOtmXr8eOsgVpcwv/hiOOecSEhPnQoPP4wrL7cw37jRRlALtn/3ndXGg2vg06dHbhGL1qqVfaGYNcsCPyMjsnTuDOedZ/stWWJfIjIyrEf6vHlxea1baVCN4447jhUrVlBYWMiFF17IOeecw2uvvcaUKVMoKysjMzOTt956i4KCAs4//3zmzp2Lc45rr72WE088kbZt21JQUADAc889x8svv8zDDz/MhAkTSEtL45NPPmHEiBGccsopXHjhhRQWFtKqVSseeugh+vXrR1lZGZdffjmvvfYaCQkJnH322ey9997ceeedvPzyywC88cYb3HPPPbz44othnioRaanqGubO2QAqbdvaQCkTJ8JTT0VmBzvvvJrf56qr4Ne/tqB++GHr6Oa9BXlOTqS2/s03FuIbNkD37pEwvugiu7ZeVRDmr71mNe8grDt0sI52wSxkq1bZfebt29vALzspyOM7jLOzt1138sl2kjdvhqOO2nb7hAm25OXBSSdV3hZMol6LBx98kIyMDLZs2cL+++/Psccey9lnn83s2bPp1asX69evB+DGG28kPT2dhQsXArBhw4Za33vlypW8//77JCYm8uOPP/Luu++SlJTEm2++yZQpU3j++eeZPn06y5YtY/78+SQlJbF+/Xo6dOjApEmTWLt2LVlZWTz00EP85je/qdPPIyISN+raAz3QpYstYEH58suRa91BKF59dWT/8nL46afI62uvtZHS1q+Hf/7TAjg6zD//3K6hb9hgt5SB9SwPwnjMmEgzfSApyWrjw4fb6Gzl5ZXDvG9fGDLE9s3Ptyb8Dz+kG3SO9WPGdxiH5O67795a41yxYgXTp09n5MiRW2/xycjIAODNN9/k6aef3npch+hZbGIYO3YsiRUjA+Xn53PGGWfw1Vdf4ZyjpOJWhjfffJNJkyZtbcYOPu+UU07h8ccf58wzz2TOnDk8+uijDfQTi4g0op3RAz2QkBCZDQxsMJVg+s6f/cym+4wO8z/+MbJvYaGFcmlpZN2NN9pEIzNnWpO491Ybz8mxz3/+eeuhXlwcOWbCBLsdzXu7J7y0FLynM3SL9aPFdxjXVJNt3brm7ZmZda4JV/7IHN58803mzJlD69atyc7OZp999uGLL76o83tE90KuOoBJmzZttj6/+uqrGT16NC+++CLLli0ju7qWgCinn34648ePJy0tjbFjx+qas4i0PDtztLW0tEgtPDB2rD0ecEDlHujB3+slSyx0t2yxIF+/HoK/8+XldgvYyy/bZwY90auREHNLC5Wfn0+HDh1o3bo1X3zxBR988AGFhYXMnj2bb7/9FmBrM/Xhhx/O1KlTtx4bNFN36tSJxYsXU15eXuM13fz8fLp1sy9KDz/88Nb1hx9+OPfddx+lFd/Ogs/r0qULXbt25aabbuLMM89suB9aRKSlCJqWt7dWfuON9hh9vHNWQezWDQYNigzuEkwHevPNkJaGh5hprDCu4sgjj6S0tJT+/ftzxRVXMGzYMLKyspg+fTonnHACQ4YMYdy4cQBcddVVbNiwgYEDBzJkyBBmzZoFwG233cbRRx/NQQcdRJeq37Ki/OEPf+CPf/wjQ4cO3Rq8ABMnTqRHjx4MHjyYIUOG8OSTT27ddtppp7HbbrvRP5jnVUREGscOBvkP8H2sXZyvodq8dSfnjgT+BiQCD3jvb6uy/RJgIlAKrAV+471fXtN79uvXz39Z5aL44sWLFTI12LRp09bwPuussxrlM5v6v0lOTk6tzf9Smc5Z/emc1V9LPGfOuXne+/2q21Zrzdg5lwhMBX4BDADGO+eqzkX2CbCf934w8Bzw5x0rslRn5MiRLFiwgNOrGxpPRESarLr0ADoA+Np7vxTAOfc0cCywKNjBez8rav8PAKXFTjB79uxGmyhCREQaT13CuBuwIur1SuDAGvY/C6jmDmtwzp0DnAOQlZVFTpXezunp6WzatKkORWqZysrKGv38FBYWbvPv1JQUFBQ06fKHQees/nTO6k/nrLIGvTfGOXc6sB8wqrrt3vvpwHSwa8ZVrxcsXrxYNb8aNOYUioG0tDSGDh3aqJ/ZkFridakdpXNWfzpn9adzVlldwngVsFvU6+4V6ypxzh0GXAmM8t4XNUzxREREmr+63Nr0EdDHOdfLOZcCnALMjN7BOTcUuA84xnuf2/DFFBERab5qDWPvfSnwO+B1YDEww3v/uXPuBufcMRW73Q60BZ51zs13zs2M8XZxr23btmEXQUREWpg6XTP23r8KvFpl3TVRzw9r4HKJiIi0GE1/BK45c+DWW+2xAXnvueyyyxg4cCCDBg3imWeeAWD16tWMHDmSffbZh4EDB/Luu+9SVlbGhAkTtu7717/+tUHLIiIizVt8zzRQ2xSKI0bAggU2GHdCAgweDBdeuMNTKAK88MILzJ8/n08//ZS8vDz2339/Ro4cyZNPPskRRxzBlVdeSVlZGZs3b2b+/PmsWrWKzz77DICNGzdu708sIiItUNOuGefnWxCDPebnN9hbv/fee4wfP57ExEQ6derEqFGj+Oijj9h///156KGHuO6661i4cCHt2rWjd+/eLF26lPPPP5/XXnuNXXbZpcHKISIizV9814xrm0LxiScqT2n1xBORAby3cwrF2owcOZLZs2fzyiuvMGHCBC655BJ+/etf8+mnn/L6669z7733MmPGDB588MEG/2wREWmemnbNuKYprXbQIYccwjPPPENZWRlr165l9uzZHHDAASxfvpxOnTpx9tlnM3HiRD7++GPy8vIoLy/nxBNP5KabbuLjjz9usHKIiEjzF98147rYkYmma3D88cczZ84chgwZgnOOP//5z3Tu3JlHHnmE22+/neTkZNq2bcujjz7KqlWrOPPMMymvaDK/9dZbG7w8IiLSfDX9MG5gBQUFADjnuP3227n99tsrbT/jjDM444wztjlOtWEREdleTbuZWkREpBlQGIuIiIRMYSwiIhIyhbGIiEjIFMYiIiIhUxiLiIiETGEsIiISMoXxDtDcxyIi0hAUxs1AaWlp2EUQEZEdELcjcF10Ecyf37Dvuc8+cNddsbdfccUV7Lbbbvz2t78F4LrrriMpKYlZs2axYcMGSkpKuOmmmzj22GNr/ayCggKOPfbYao979NFHueOOO3DOMXjwYB577DF++OEHJk2axNKlSwGYNm0aXbt25eijj946NePdd99NSUkJ1113HdnZ2eyzzz5bZ5fq27cvN910E8XFxXTs2JEnnniCTp06UVBQwPnnn8/cuXNxznHttdeSn5/PggULuKviZNx///0sWrRI8zCLiIQkbsM4DOPGjeOiiy7aGsYzZszg9ddf54ILLmCXXXYhLy+PYcOGccwxx+Ccq/G90tLSePHFF7c5btGiRdx00028//77ZGZmsn79egAuuOACRo0axYsvvkhZWRkFBQVs2LChxs8oLi5m7ty5AGzYsIEPPvgA5xwPPPAAf/7zn/nLX/7CjTfeSHp6OgsXLty6X3JyMjfffPPWMbYfeugh7rvvvh09fSIisp3iNoxrqsHuLEOHDiU3N5fvv/+etWvX0qFDBzp37szFF1/M7NmzSUhIYNWqVfzwww907ty5xvfy3jNlypRtjnv77bcZO3YsmZmZAGRkZADw9ttv8+ijjwKQmJhIenp6rWE8bty4rc9XrlzJuHHjWL16NcXFxfTq1QuAN998k6effnrrfh06dADg0EMP5eWXX6Z///6UlJQwaNCgep4tERFpKHEbxmEZO3Yszz33HGvWrGHcuHE88cQTrF27lnnz5pGcnEzPnj0pLCys9X2297hoSUlJW2eCAigsLCQxMXHr6zZt2mx9fv7553PJJZdwzDHHkJOTw3XXXVfje0+cOJFbbrmFvfbaizPPPLNe5RIRkYalDlxVjBs3jqeffprnnnuOsWPHkp+fz6677kpycjKzZs1i+fLldXqfWMcdeuihPPvss6xbtw5gazP1mDFjmDZtGgBlZWXk5+fTqVMncnNzWbduHUVFRbz22ms1fl63bt0AeOSRR7auP/zww5k6derW10Ft+8ADD2TFihU8+eSTjB8/vq6nR0REdgKFcRV77703mzZtolu3bnTp0oXTTjuNuXPnMmjQIB599FH22muvOr1PrOP23ntvrrzySkaNGsWQIUO45JJLAPjb3/7GrFmzGDRoEPvuuy+LFi0iOTmZa665hgMOOIDDDz+cvn37xvy86667jrFjx7LvvvtubQIHuOqqq9iwYQMDBw5kyJAhzJo1a+u2k08+mREjRmxtuhYRkXA4730oH9yvXz//5ZdfVlq3ePFi+vfvH0p5moJNmzbRrl27Bnu/o48+mosvvpgxY8bE3Kep/5vk5OSQnZ0ddjGaFJ2z+tM5q7+WeM6cc/O89/tVt0014xZo48aN9O3bl1atWtUYxCIi0jjUgWsHLVy4kF/96leV1qWmpvLhhx+GVKLatW/fniVLloRdDBERqaAw3kGDBg1ifkOPTiIiIi2KmqlFRERCpjAWEREJmcJYREQkZApjERGRkDX5MF69GkaNgjVrGv+za5rPeNmyZQwcOLARSyMiIk1Vkw/jG2+E996DG24IuyQiIiLbJ25vbaptPuN334WoORSYNs2WhAQ45JDqj2nM+YyjFRYWMnnyZObOnUtSUhJ33nkno0eP5vPPP+fMM8+kuLiY8vJynn/+ebp27crJJ5/MypUrKSsr4+qrr640O5OIiDQ/cRvGtTngAFi6FPLyLJQTEiAzE/bYY/vfsyHnM442depUnHMsXLiQL774gp///OcsWbKEe++9lwsvvJDTTjuN4uJiysrKePXVV+natSuvvPIKYBNAiIhI8xa3YVyX+YwnT4bp0yEtDYqL4cQT4Z57tv8zG3I+42jvvfce559/PgB77bUXu+++O0uWLGH48OHcfPPNrFy5khNOOIE+ffowaNAgLr30Ui6//HKOPvpoDolVzRcRkWajSV8z/uEHmDQJPvjAHhuiE1cwn/EzzzyzzXzG8+fPp1OnTvWelziWU089lZkzZ9KqVSuOOuoo3n77bfr27cvHH3/MoEGDuOqqq7hBF8NFRJq9uK0Z18ULL0SeR03Zu0PGjRvH2WefTV5eHu+88w4zZszYrvmMox1yyCE88cQTHHrooSxZsoTvvvuOfv36sXTpUnr37s0FF1zAd999x4IFC9hrr73IyMjg9NNPp3379jzwwAMN84OJiEjcatJhvDNUN5/xL3/5SwYNGsR+++1X5/mMo5133nlMnjyZQYMGkZSUxMMPP0xqaiozZszgscceIzk5mc6dOzNlyhQ++ugjLrvsMhISEkhOTmbatGk74acUEZF4ojCuxsKFC7c+z8zMZM6cOdXuV1BQEPM9evbsyWeffQZAWloaDz300Db7XHHFFVxxxRWV1h1xxBEcccQR21NsERFpopr0NWMREZHmQDXjHdQU5zMWEZH4ojDeQZrPWEREdlTcNVN778MuglTQv4WISOOIqzBOS0tj3bp1CoE44L1n3bp1pKWlhV0UEZFmL66aqbt3787KlStZu3Zt2EWJS4WFhY0ajmlpaXTv3r3RPk9EpKWqUxg7544E/gYkAg9472+rsj0VeBTYF1gHjPPeL6tvYZKTk+nVq1d9D2sxcnJyGDp0aNjFEBGRBlZrM7VzLhGYCvwCGACMd84NqLLbWcAG7/2ewF+BPzV0QUVERJqrulwzPgD42nu/1HtfDDwNVJ1D8FjgkYrnzwFjXH2mNRIREWnB6hLG3YAVUa9XVqyrdh/vfSmQD3RsiAKKiIg0d43agcs5dw5wTsXLIufcZ435+c1AJpAXdiGaGJ2z+tM5qz+ds/prieds91gb6hLGq4Ddol53r1hX3T4rnXNJQDrWkasS7/10YDqAc26u936/Ony+VNA5qz+ds/rTOas/nbP60zmrrC7N1B8BfZxzvZxzKcApwMwq+8wEzqh4fhLwttfNwiIiInVSa83Ye1/qnPsd8Dp2a9OD3vvPnXM3AHO99zOBfwCPOee+BtZjgS0iIiJ1UKdrxt77V4FXq6y7Jup5ITC2np89vZ77i87Z9tA5qz+ds/rTOas/nbMoTq3JIiIi4YqrsalFRERaolDC2Dl3pHPuS+fc1865K8IoQ1PjnFvmnFvonJvvnJsbdnnikXPuQedcbvQtc865DOfcG865ryoeO4RZxngT45xd55xbVfG7Nt85d1SYZYwnzrndnHOznHOLnHOfO+curFiv37MYajhn+j2L0ujN1BXDay4BDscGEPkIGO+9X9SoBWlinHPLgP289y3tvrw6c86NBAqAR733AyvW/RlY772/reKLXwfv/eVhljOexDhn1wEF3vs7wixbPHLOdQG6eO8/ds61A+YBxwET0O9ZtWo4Zyej37OtwqgZ12V4TZF6897PxnrzR4seqvUR7I+AVIhxziQG7/1q7/3HFc83AYuxEQj1exZDDedMooQRxnUZXlO25YH/OOfmVYxkJnXTyXu/uuL5GqBTmIVpQn7nnFtQ0YytJtdqOOd6AkOBD9HvWZ1UOWeg37Ot1IGr6TjYe/8zbPas31Y0L0o9VAxEo9sHajcN2APYB1gN/CXU0sQh51xb4HngIu/9j9Hb9HtWvWrOmX7PooQRxnUZXlOq8N6vqnjMBV7Emvuldj9UXLMKrl3lhlyeuOe9/8F7X+a9LwfuR79rlTjnkrFQecJ7/0LFav2e1aC6c6bfs8rCCOO6DK8pUZxzbSo6PuCcawP8HNAkG3UTPVTrGcC/QixLkxCESoXj0e/aVhVTw/4DWOy9vzNqk37PYoh1zvR7Vlkog35UdGG/i8jwmjc3eiGaEOdcb6w2DDZq2pM6Z9tyzj0FZGOzwfwAXAv8E5gB9ACWAyd779VhqUKMc5aNNR16YBlwbtT10BbNOXcw8C6wECivWD0Fuwaq37Nq1HDOxqPfs600ApeIiEjI1IFLREQkZApjERGRkCmMRUREQqYwFhERCZnCWEREJGQKY5EmyjlXFjXjzfyGnAHNOdczeiYnEdm5ksIugIhsty3e+33CLoSI7DjVjEWamYq5r/9cMf/1/5xze1as7+mce7tiYP63nHM9KtZ3cs696Jz7tGI5qOKtEp1z91fMQfsf51yr0H4okWZOYSzSdLWq0kw9Lmpbvvd+EPB3bLQ7gP8DHvHeDwaeAO6uWH838I73fgjwM+DzivV9gKne+72BjcCJO/WnEWnBNAKXSBPlnCvw3retZv0y4FDv/dKKAfrXeO87OufysEneSyrWr/beZzrn1gLdvfdFUe/RE3jDe9+n4vXlQLL3/qZG+NFEWhzVjEWaJx/jeX0URT0vQ31MRHYahbFI8zQu6nFOxfP3sVnSAE7DBu8HeAuYDOCcS3TOpTdWIUXE6JuuSNPVyjk3P+r1a9774PamDs65BVjtdnzFuvOBh5xzlwFrgTMr1l8ITHfOnYXVgCdjk72LSCPRNWORZqbimvF+3vu8sMsiInWjZmoREZGQqWYsIiISMtWMRUREQqYwFhERCZnCWEREJGQKYxERkZApjEVEREKmMBYREQnZ/wcmMpZqe27+MwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8, 5),\n",
    "    xlim=[0, 29],\n",
    "    ylim=[0, 1],\n",
    "    grid=True,\n",
    "    xlabel=\"Epoch\",\n",
    "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8709 - loss: 0.3662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3637225031852722, 0.8725000023841858]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model to Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.17, 0.  , 0.01, 0.  , 0.82],\n",
       "       [0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you only care about the class with the highest estimated probability (even if that probability is quite low), then you can use the argmax() method to get the highest probability class index for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = y_proba.argmax(axis=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Regression MLP Using the Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let’s switch back to the California housing problem and tackle it using the same MLP as earlier, with 3 hidden layers composed of 50 neurons each, but this time building it with Keras. Using the sequential API to build, train, evaluate, and use a regression MLP is quite similar to what we did for classification. The main differences in the following code example are the fact that the output layer has a single neuron (since we only want to predict a single value) and it uses no activation function, the loss function is the mean squared error, the metric is the RMSE, and we’re using an Adam optimizer like Scikit-Learn’s MLPRegressor did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MRE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - RootMeanSquaredError: 1.3557 - loss: 1.9923 - val_RootMeanSquaredError: 0.6215 - val_loss: 0.3862\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - RootMeanSquaredError: 0.6311 - loss: 0.3986 - val_RootMeanSquaredError: 0.6407 - val_loss: 0.4104\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - RootMeanSquaredError: 0.6009 - loss: 0.3613 - val_RootMeanSquaredError: 0.8348 - val_loss: 0.6969\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - RootMeanSquaredError: 0.5888 - loss: 0.3468 - val_RootMeanSquaredError: 1.0563 - val_loss: 1.1158\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - RootMeanSquaredError: 0.5813 - loss: 0.3380 - val_RootMeanSquaredError: 0.6949 - val_loss: 0.4829\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - RootMeanSquaredError: 0.5713 - loss: 0.3266 - val_RootMeanSquaredError: 0.7941 - val_loss: 0.6306\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - RootMeanSquaredError: 0.5655 - loss: 0.3199 - val_RootMeanSquaredError: 0.7476 - val_loss: 0.5589\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - RootMeanSquaredError: 0.5597 - loss: 0.3134 - val_RootMeanSquaredError: 0.6857 - val_loss: 0.4702\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - RootMeanSquaredError: 0.5549 - loss: 0.3080 - val_RootMeanSquaredError: 0.5366 - val_loss: 0.2879\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - RootMeanSquaredError: 0.5472 - loss: 0.2995 - val_RootMeanSquaredError: 0.5487 - val_loss: 0.3010\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - RootMeanSquaredError: 0.5425 - loss: 0.2944 - val_RootMeanSquaredError: 0.5367 - val_loss: 0.2880\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - RootMeanSquaredError: 0.5391 - loss: 0.2907 - val_RootMeanSquaredError: 0.5827 - val_loss: 0.3395\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - RootMeanSquaredError: 0.5357 - loss: 0.2870 - val_RootMeanSquaredError: 0.5772 - val_loss: 0.3332\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - RootMeanSquaredError: 0.5330 - loss: 0.2841 - val_RootMeanSquaredError: 0.5479 - val_loss: 0.3002\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - RootMeanSquaredError: 0.5297 - loss: 0.2807 - val_RootMeanSquaredError: 0.6497 - val_loss: 0.4221\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - RootMeanSquaredError: 0.5276 - loss: 0.2784 - val_RootMeanSquaredError: 0.6360 - val_loss: 0.4045\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - RootMeanSquaredError: 0.5235 - loss: 0.2741 - val_RootMeanSquaredError: 0.6255 - val_loss: 0.3913\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - RootMeanSquaredError: 0.5214 - loss: 0.2719 - val_RootMeanSquaredError: 0.6793 - val_loss: 0.4615\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - RootMeanSquaredError: 0.5203 - loss: 0.2707 - val_RootMeanSquaredError: 0.5267 - val_loss: 0.2774\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - RootMeanSquaredError: 0.5172 - loss: 0.2676 - val_RootMeanSquaredError: 0.7917 - val_loss: 0.6268\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - RootMeanSquaredError: 0.5280 - loss: 0.2790\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\n",
    "model = tf.keras.Sequential([\n",
    "    norm_layer,\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
    "norm_layer.adapt(X_train)\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    validation_data=(X_valid, y_valid)\n",
    ")\n",
    "\n",
    "mse_test, rmse_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Complex Models Using the Functional API\n",
    "\n",
    "* It connects all or part of the inputs directly to the output layer. This architecture makes it possible for the neural network to learn both deep patterns (using the deep path) and simple rules (through the short path). In contrast, a regular MLP forces all the data to flow through the full stack of layers; thus, simple patterns in the data may end up being distorted by this sequence of transformations.\n",
    "\n",
    "* Let’s build such a neural network to tackle the California housing problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Normalization()\n",
    "\n",
    "hidden_layer1 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "hidden_layer2 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "concat_layer = tf.keras.layers.Concatenate()\n",
    "output_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "input_ = tf.keras.layers.Input(shape=X_train.shape[1:])\n",
    "normalized = normalization_layer(input_)\n",
    "\n",
    "hidden1 = hidden_layer1(normalized)\n",
    "hidden2 = hidden_layer2(hidden1)\n",
    "\n",
    "concat = concat_layer([normalized, hidden2])\n",
    "\n",
    "output = output_layer(concat)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_],\n",
    "    outputs=[output]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* But what if you want to send a subset of the features through the wide path and a different subset (possibly overlapping) through the deep path ? In this case, one solution is to use multiple inputs. For example, suppose we want to send five features through the wide path (features 0 to 4), and six features through the deep path (features 2 to 7). We can do this as follows.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_wide = tf.keras.layers.Input(shape=[5]) # features 0 to 4\n",
    "input_deep = tf.keras.layers.Input(shape=[6]) # features 2 to 7\n",
    "\n",
    "norm_layer_wide = tf.keras.layers.Normalization()\n",
    "norm_layer_deep = tf.keras.layers.Normalization()\n",
    "\n",
    "norm_wide = norm_layer_wide(input_wide)\n",
    "norm_deep = norm_layer_deep(input_deep)\n",
    "\n",
    "hidden1 = tf.keras.layers.Dense(30, activation=\"relu\")(norm_deep)\n",
    "hidden2 = tf.keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "\n",
    "concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
    "output = tf.keras.layers.Dense(1)(concat)\n",
    "model = tf.keras.Model(inputs=[input_wide, input_deep], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we can compile the model as usual, but when we call the fit() method, instead of passing a single input matrix X_train, we must pass a pair of matrices (X_train_wide, X_train_deep), one per input. The same is true for X_valid, and also for X_test and X_new when you call evaluate() or predict():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - RootMeanSquaredError: 1.5471 - loss: 2.5124 - val_RootMeanSquaredError: 1.1951 - val_loss: 1.4282\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - RootMeanSquaredError: 0.7475 - loss: 0.5595 - val_RootMeanSquaredError: 0.6575 - val_loss: 0.4323\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - RootMeanSquaredError: 0.6762 - loss: 0.4575 - val_RootMeanSquaredError: 0.6786 - val_loss: 0.4605\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - RootMeanSquaredError: 0.6517 - loss: 0.4249 - val_RootMeanSquaredError: 0.6195 - val_loss: 0.3838\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - RootMeanSquaredError: 0.6401 - loss: 0.4099 - val_RootMeanSquaredError: 0.7228 - val_loss: 0.5224\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - RootMeanSquaredError: 0.6316 - loss: 0.3990 - val_RootMeanSquaredError: 0.7065 - val_loss: 0.4992\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - RootMeanSquaredError: 0.6256 - loss: 0.3915 - val_RootMeanSquaredError: 1.1159 - val_loss: 1.2453\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - RootMeanSquaredError: 0.6221 - loss: 0.3872 - val_RootMeanSquaredError: 1.3222 - val_loss: 1.7483\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - RootMeanSquaredError: 0.6192 - loss: 0.3835 - val_RootMeanSquaredError: 1.7092 - val_loss: 2.9215\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - RootMeanSquaredError: 0.6171 - loss: 0.3810 - val_RootMeanSquaredError: 1.3723 - val_loss: 1.8832\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - RootMeanSquaredError: 0.6109 - loss: 0.3733 - val_RootMeanSquaredError: 0.8978 - val_loss: 0.8060\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - RootMeanSquaredError: 0.6030 - loss: 0.3637 - val_RootMeanSquaredError: 0.9668 - val_loss: 0.9347\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - RootMeanSquaredError: 0.6009 - loss: 0.3611 - val_RootMeanSquaredError: 1.2154 - val_loss: 1.4773\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - RootMeanSquaredError: 0.5974 - loss: 0.3569 - val_RootMeanSquaredError: 1.5293 - val_loss: 2.3389\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - RootMeanSquaredError: 0.5971 - loss: 0.3566 - val_RootMeanSquaredError: 1.7317 - val_loss: 2.9989\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - RootMeanSquaredError: 0.5954 - loss: 0.3546 - val_RootMeanSquaredError: 1.7991 - val_loss: 3.2368\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - RootMeanSquaredError: 0.5940 - loss: 0.3530 - val_RootMeanSquaredError: 1.5069 - val_loss: 2.2708\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - RootMeanSquaredError: 0.5922 - loss: 0.3507 - val_RootMeanSquaredError: 1.0460 - val_loss: 1.0940\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - RootMeanSquaredError: 0.5838 - loss: 0.3409 - val_RootMeanSquaredError: 0.8177 - val_loss: 0.6687\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - RootMeanSquaredError: 0.5809 - loss: 0.3375 - val_RootMeanSquaredError: 0.8531 - val_loss: 0.7277\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - RootMeanSquaredError: 0.5834 - loss: 0.3404\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453ms/step\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
    "\n",
    "X_train_wide, X_train_deep = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_wide, X_valid_deep = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_wide, X_test_deep = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_wide, X_new_deep = X_test_wide[:3], X_test_deep[:3]\n",
    "\n",
    "norm_layer_wide.adapt(X_train_wide)\n",
    "norm_layer_deep.adapt(X_train_deep)\n",
    "\n",
    "history = model.fit((X_train_wide,X_train_deep), y_train, epochs=20, validation_data=((X_valid_wide,X_valid_deep),y_valid))\n",
    "\n",
    "mse_test = model.evaluate((X_test_wide, X_test_deep), y_test)\n",
    "y_pred = model.predict((X_new_wide, X_new_deep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adding an auxiliary output for regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\MRE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_wide = tf.keras.layers.Input(shape=[5])  # features 0 to 4\n",
    "input_deep = tf.keras.layers.Input(shape=[6])  # features 2 to 7\n",
    "\n",
    "norm_layer_wide = tf.keras.layers.Normalization()\n",
    "norm_layer_deep = tf.keras.layers.Normalization()\n",
    "\n",
    "norm_wide = norm_layer_wide(input_wide)\n",
    "norm_deep = norm_layer_deep(input_deep)\n",
    "\n",
    "hidden1 = tf.keras.layers.Dense(30, activation=\"relu\")(norm_deep)\n",
    "hidden2 = tf.keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
    "output = tf.keras.layers.Dense(1)(concat)\n",
    "aux_output = tf.keras.layers.Dense(1)(hidden2)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_wide, input_deep], \n",
    "    outputs=[output, aux_output]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each output will need its own loss function. Therefore, when we compile the model, we should pass a list of losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "model.compile(\n",
    "    loss=(\"mse\", \"mse\"),\n",
    "    loss_weights=(0.9, 0.1),\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"RootMeanSquaredError\", \"RootMeanSquaredError\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now when we train the model, we need to provide labels for each output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - dense_2_RootMeanSquaredError: 1.5071 - dense_2_loss: 2.3754 - dense_3_RootMeanSquaredError: 2.0094 - dense_3_loss: 4.1195 - loss: 2.5498 - val_dense_2_RootMeanSquaredError: 0.7452 - val_dense_2_loss: 0.5552 - val_dense_3_RootMeanSquaredError: 4.0299 - val_dense_3_loss: 16.2322 - val_loss: 2.1238\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - dense_2_RootMeanSquaredError: 0.7309 - dense_2_loss: 0.5350 - dense_3_RootMeanSquaredError: 0.9962 - dense_3_loss: 0.9934 - loss: 0.5809 - val_dense_2_RootMeanSquaredError: 0.7132 - val_dense_2_loss: 0.5085 - val_dense_3_RootMeanSquaredError: 1.7374 - val_dense_3_loss: 3.0173 - val_loss: 0.7596\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - dense_2_RootMeanSquaredError: 0.6742 - dense_2_loss: 0.4549 - dense_3_RootMeanSquaredError: 0.8330 - dense_3_loss: 0.6941 - loss: 0.4788 - val_dense_2_RootMeanSquaredError: 0.9765 - val_dense_2_loss: 0.9531 - val_dense_3_RootMeanSquaredError: 1.1298 - val_dense_3_loss: 1.2760 - val_loss: 0.9858\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - dense_2_RootMeanSquaredError: 0.6563 - dense_2_loss: 0.4310 - dense_3_RootMeanSquaredError: 0.7847 - dense_3_loss: 0.6158 - loss: 0.4495 - val_dense_2_RootMeanSquaredError: 0.7196 - val_dense_2_loss: 0.5177 - val_dense_3_RootMeanSquaredError: 0.7763 - val_dense_3_loss: 0.6025 - val_loss: 0.5264\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - dense_2_RootMeanSquaredError: 0.6457 - dense_2_loss: 0.4172 - dense_3_RootMeanSquaredError: 0.7606 - dense_3_loss: 0.5786 - loss: 0.4333 - val_dense_2_RootMeanSquaredError: 1.6694 - val_dense_2_loss: 2.7856 - val_dense_3_RootMeanSquaredError: 0.7458 - val_dense_3_loss: 0.5562 - val_loss: 2.5638\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - dense_2_RootMeanSquaredError: 0.6396 - dense_2_loss: 0.4093 - dense_3_RootMeanSquaredError: 0.7441 - dense_3_loss: 0.5538 - loss: 0.4238 - val_dense_2_RootMeanSquaredError: 1.8447 - val_dense_2_loss: 3.4014 - val_dense_3_RootMeanSquaredError: 1.0318 - val_dense_3_loss: 1.0641 - val_loss: 3.1692\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - dense_2_RootMeanSquaredError: 0.6319 - dense_2_loss: 0.3994 - dense_3_RootMeanSquaredError: 0.7217 - dense_3_loss: 0.5210 - loss: 0.4116 - val_dense_2_RootMeanSquaredError: 1.7775 - val_dense_2_loss: 3.1581 - val_dense_3_RootMeanSquaredError: 0.8025 - val_dense_3_loss: 0.6439 - val_loss: 2.9080\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - dense_2_RootMeanSquaredError: 0.6208 - dense_2_loss: 0.3855 - dense_3_RootMeanSquaredError: 0.7046 - dense_3_loss: 0.4966 - loss: 0.3966 - val_dense_2_RootMeanSquaredError: 1.5517 - val_dense_2_loss: 2.4068 - val_dense_3_RootMeanSquaredError: 1.0781 - val_dense_3_loss: 1.1618 - val_loss: 2.2833\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - dense_2_RootMeanSquaredError: 0.6120 - dense_2_loss: 0.3746 - dense_3_RootMeanSquaredError: 0.6819 - dense_3_loss: 0.4652 - loss: 0.3837 - val_dense_2_RootMeanSquaredError: 1.5145 - val_dense_2_loss: 2.2925 - val_dense_3_RootMeanSquaredError: 0.9700 - val_dense_3_loss: 0.9405 - val_loss: 2.1583\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - dense_2_RootMeanSquaredError: 0.6052 - dense_2_loss: 0.3664 - dense_3_RootMeanSquaredError: 0.6702 - dense_3_loss: 0.4493 - loss: 0.3747 - val_dense_2_RootMeanSquaredError: 1.4720 - val_dense_2_loss: 2.1659 - val_dense_3_RootMeanSquaredError: 1.2993 - val_dense_3_loss: 1.6874 - val_loss: 2.1190\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - dense_2_RootMeanSquaredError: 0.5992 - dense_2_loss: 0.3591 - dense_3_RootMeanSquaredError: 0.6552 - dense_3_loss: 0.4294 - loss: 0.3661 - val_dense_2_RootMeanSquaredError: 1.4281 - val_dense_2_loss: 2.0384 - val_dense_3_RootMeanSquaredError: 1.0739 - val_dense_3_loss: 1.1528 - val_loss: 1.9508\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - dense_2_RootMeanSquaredError: 0.5953 - dense_2_loss: 0.3545 - dense_3_RootMeanSquaredError: 0.6513 - dense_3_loss: 0.4244 - loss: 0.3615 - val_dense_2_RootMeanSquaredError: 1.4003 - val_dense_2_loss: 1.9599 - val_dense_3_RootMeanSquaredError: 1.3244 - val_dense_3_loss: 1.7532 - val_loss: 1.9401\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - dense_2_RootMeanSquaredError: 0.5921 - dense_2_loss: 0.3507 - dense_3_RootMeanSquaredError: 0.6425 - dense_3_loss: 0.4130 - loss: 0.3569 - val_dense_2_RootMeanSquaredError: 1.3064 - val_dense_2_loss: 1.7059 - val_dense_3_RootMeanSquaredError: 1.0493 - val_dense_3_loss: 1.1006 - val_loss: 1.6462\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - dense_2_RootMeanSquaredError: 0.5880 - dense_2_loss: 0.3458 - dense_3_RootMeanSquaredError: 0.6387 - dense_3_loss: 0.4081 - loss: 0.3520 - val_dense_2_RootMeanSquaredError: 1.1725 - val_dense_2_loss: 1.3741 - val_dense_3_RootMeanSquaredError: 0.9835 - val_dense_3_loss: 0.9670 - val_loss: 1.3339\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - dense_2_RootMeanSquaredError: 0.5844 - dense_2_loss: 0.3415 - dense_3_RootMeanSquaredError: 0.6312 - dense_3_loss: 0.3985 - loss: 0.3472 - val_dense_2_RootMeanSquaredError: 1.0899 - val_dense_2_loss: 1.1875 - val_dense_3_RootMeanSquaredError: 0.7702 - val_dense_3_loss: 0.5931 - val_loss: 1.1285\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - dense_2_RootMeanSquaredError: 0.5825 - dense_2_loss: 0.3394 - dense_3_RootMeanSquaredError: 0.6286 - dense_3_loss: 0.3953 - loss: 0.3450 - val_dense_2_RootMeanSquaredError: 1.0787 - val_dense_2_loss: 1.1630 - val_dense_3_RootMeanSquaredError: 0.8795 - val_dense_3_loss: 0.7733 - val_loss: 1.1245\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - dense_2_RootMeanSquaredError: 0.5793 - dense_2_loss: 0.3357 - dense_3_RootMeanSquaredError: 0.6245 - dense_3_loss: 0.3902 - loss: 0.3411 - val_dense_2_RootMeanSquaredError: 1.0250 - val_dense_2_loss: 1.0502 - val_dense_3_RootMeanSquaredError: 0.7147 - val_dense_3_loss: 0.5106 - val_loss: 0.9966\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - dense_2_RootMeanSquaredError: 0.5764 - dense_2_loss: 0.3323 - dense_3_RootMeanSquaredError: 0.6203 - dense_3_loss: 0.3850 - loss: 0.3376 - val_dense_2_RootMeanSquaredError: 1.0666 - val_dense_2_loss: 1.1371 - val_dense_3_RootMeanSquaredError: 1.1409 - val_dense_3_loss: 1.3010 - val_loss: 1.1540\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - dense_2_RootMeanSquaredError: 0.5745 - dense_2_loss: 0.3301 - dense_3_RootMeanSquaredError: 0.6187 - dense_3_loss: 0.3829 - loss: 0.3354 - val_dense_2_RootMeanSquaredError: 1.0739 - val_dense_2_loss: 1.1527 - val_dense_3_RootMeanSquaredError: 1.1580 - val_dense_3_loss: 1.3405 - val_loss: 1.1720\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - dense_2_RootMeanSquaredError: 0.5736 - dense_2_loss: 0.3291 - dense_3_RootMeanSquaredError: 0.6194 - dense_3_loss: 0.3838 - loss: 0.3346 - val_dense_2_RootMeanSquaredError: 1.1022 - val_dense_2_loss: 1.2144 - val_dense_3_RootMeanSquaredError: 1.0164 - val_dense_3_loss: 1.0326 - val_loss: 1.1967\n"
     ]
    }
   ],
   "source": [
    "norm_layer_wide.adapt(X_train_wide)\n",
    "norm_layer_deep.adapt(X_train_deep)\n",
    "history = model.fit(\n",
    "    (X_train_wide, X_train_deep),\n",
    "    (y_train, y_train),\n",
    "    epochs=20,\n",
    "    validation_data=(\n",
    "        (X_valid_wide, X_valid_deep),\n",
    "        (y_valid, y_valid)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - dense_2_RootMeanSquaredError: 0.5755 - dense_2_loss: 0.3314 - dense_3_RootMeanSquaredError: 0.6211 - dense_3_loss: 0.3859 - loss: 0.3368\n"
     ]
    }
   ],
   "source": [
    "eval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))\n",
    "\n",
    "weighted_sum_of_losses, main_loss, aux_loss, main_rmse, aux_rmse = eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The predict() method returns a tuple, and it does not have a return_dict argument to get a dictionary instead. However, you can create one using model.output_names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_tuple = model.predict((X_new_wide, X_new_deep))\n",
    "y_pred = dict(zip(model.output_names, y_pred_tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Subclassing API to Build Dynamic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(tf.keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # needed to support naming the model\n",
    "        self.norm_layer_wide = tf.keras.layers.Normalization()\n",
    "        self.norm_layer_deep = tf.keras.layers.Normalization()\n",
    "        self.hidden1 = tf.keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = tf.keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = tf.keras.layers.Dense(1)\n",
    "        self.aux_output = tf.keras.layers.Dense(1)\n",
    "    def call(self, inputs):\n",
    "        input_wide, input_deep = inputs\n",
    "        norm_wide = self.norm_layer_wide(input_wide)\n",
    "        norm_deep = self.norm_layer_deep(input_deep)\n",
    "        hidden1 = self.hidden1(norm_deep)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
    "        output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return output, aux_output\n",
    "\n",
    "model = WideAndDeepModel(30, activation=\"relu\", name=\"my_cool_model\")\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
